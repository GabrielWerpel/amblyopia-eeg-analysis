{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mne scipy\n",
    "#!pip install pandas numpy openpyxl\n",
    "#!pip install tsfresh\n",
    "#!pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import mne\n",
    "\n",
    "def process_all_eeg_data() -> dict:\n",
    "    \"\"\"\n",
    "    Process all .bdf EEG files in the current directory, applying filters and extracting data from\n",
    "    channels A15 (O1), A16 (Oz), and A17 (O2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing processed EEG data and header information for each file.\n",
    "    \"\"\"\n",
    "    # Get a list of all .bdf files in the current directory\n",
    "    files = [f for f in os.listdir('.') if f.endswith('.bdf')]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No BDF files found in the current directory\")\n",
    "    \n",
    "    # Initialize the results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Loop over each file\n",
    "    for filename in files:\n",
    "        full_file_path = os.path.join(os.getcwd(), filename)\n",
    "        \n",
    "        # Read the raw EEG data using MNE\n",
    "        raw = mne.io.read_raw_bdf(full_file_path, preload=True)\n",
    "        hdr = raw.info\n",
    "        \n",
    "        # Select data from channels A15 (O1), A16 (Oz), and A17 (O2)\n",
    "        channels_select = ['A15', 'A16', 'A17']\n",
    "        missing_channels = [ch for ch in channels_select if ch not in hdr['ch_names']]\n",
    "        if missing_channels:\n",
    "            raise ValueError(f\"Selected channels {missing_channels} not found in the data\")\n",
    "        \n",
    "        channel_indices = [hdr['ch_names'].index(ch) for ch in channels_select]\n",
    "        EEG_data = raw.get_data(picks=channel_indices).T  # Shape: (n_samples, n_channels)\n",
    "        \n",
    "        # Filter EEG Data\n",
    "        Fs = hdr['sfreq']  # Sampling frequency\n",
    "        \n",
    "        # Bandpass filter parameters (2 to 80 Hz)\n",
    "        Fc_BP = [2, 80]  # Bandpass frequency range\n",
    "        Wn_BP = [f / (Fs / 2) for f in Fc_BP]  # Normalize by Nyquist frequency\n",
    "        \n",
    "        # Create and apply bandpass filter (6th order zero-phase Butterworth IIR)\n",
    "        B_BP, A_BP = signal.butter(3, Wn_BP, btype='bandpass')\n",
    "        EEG_filtered_BP = signal.filtfilt(B_BP, A_BP, EEG_data, axis=0)\n",
    "        \n",
    "        # Band stop filter parameters (48 to 52 Hz)\n",
    "        Fc_BS = [48, 52]  # Band stop frequency range\n",
    "        Wn_BS = [f / (Fs / 2) for f in Fc_BS]  # Normalize by Nyquist frequency\n",
    "        \n",
    "        # Create and apply band stop filter (6th order zero-phase Butterworth IIR)\n",
    "        B_BS, A_BS = signal.butter(3, Wn_BS, btype='bandstop')\n",
    "        EEG_filtered = signal.filtfilt(B_BS, A_BS, EEG_filtered_BP, axis=0)\n",
    "        \n",
    "        # Extract prefix before underscore from the filename\n",
    "        underscore_index = filename.find('_')\n",
    "        if underscore_index == -1:\n",
    "            raise ValueError(f\"Filename format error, no underscore found in {filename}\")\n",
    "        key = filename[:underscore_index]\n",
    "        \n",
    "        # Store results in the dictionary\n",
    "        results[key] = {\n",
    "            'data': EEG_filtered,      # Filtered data for channels A15, A16, A17\n",
    "            'channels': channels_select,  # List of channel names\n",
    "            'header': hdr\n",
    "        }\n",
    "        \n",
    "        # Display a message indicating successful processing\n",
    "        print(f\"Data for file {filename} processed successfully\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A1_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 739327  =      0.000 ...   361.000 secs...\n",
      "Data for file A1_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A3_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 757759  =      0.000 ...   370.000 secs...\n",
      "Data for file A3_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A4_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 782335  =      0.000 ...   382.000 secs...\n",
      "Data for file A4_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A6_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A6_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A7_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A7_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A8_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A8_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A9_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A9_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C11_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C11_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C12_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C12_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C13_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C13_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C14_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C14_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C15_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C15_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C1_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C1_Alpha.bdf processed successfully\n"
     ]
    }
   ],
   "source": [
    "results = process_all_eeg_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def segment_eeg_data_new(results: dict, cohort_file: str = 'Cohort.xlsx') -> dict:\n",
    "    \"\"\"\n",
    "    Segments EEG data into predefined sections (EC, EO, LC, RC, DEC, NDEC) based on cohort information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        Dictionary containing the raw EEG data and header information for each key (participant).\n",
    "    cohort_file : str, optional\n",
    "        Path to the Excel file containing cohort information (default is 'Cohort.xlsx').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing segmented EEG data for each participant.\n",
    "    \"\"\"\n",
    "    # Read the cohort information from an Excel file\n",
    "    cohort_table = pd.read_excel(cohort_file)\n",
    "    # Segment Duration (in seconds)\n",
    "    segment_duration = 10  # seconds\n",
    "\n",
    "    # Initialize the segmented results dictionary\n",
    "    segmented_data = {}\n",
    "\n",
    "    # Iterate through each key in the results dictionary\n",
    "    for key, result in results.items():\n",
    "        data = result['data']  # data shape: (n_samples, n_channels)\n",
    "        hdr = result['header']\n",
    "\n",
    "        # Find the matching row in the cohort table\n",
    "        cohort_row = cohort_table[cohort_table['Cohort'] == key]\n",
    "        \n",
    "        if cohort_row.empty:\n",
    "            raise ValueError(f\"Cohort information not found for {key}\")\n",
    "\n",
    "        # Define the duration and sample rate\n",
    "        samples_per_segment = int(segment_duration * hdr['sfreq'])\n",
    "        n_channels = data.shape[1]  # Number of channels (should be 3: O1, Oz, O2)\n",
    "\n",
    "        # Initialize segments with zeros\n",
    "        EC = np.zeros((samples_per_segment, n_channels))\n",
    "        EO = np.zeros((samples_per_segment, n_channels))\n",
    "        LC = np.zeros((samples_per_segment, n_channels))\n",
    "        RC = np.zeros((samples_per_segment, n_channels))\n",
    "        DEC = np.zeros((samples_per_segment, n_channels))\n",
    "        NDEC = np.zeros((samples_per_segment, n_channels))\n",
    "\n",
    "        # Fill segments with data if available\n",
    "        if data.shape[0] >= samples_per_segment:\n",
    "            EC = data[:samples_per_segment, :]\n",
    "        if data.shape[0] >= 2 * samples_per_segment:\n",
    "            EO = data[samples_per_segment:2 * samples_per_segment, :]\n",
    "        if data.shape[0] >= 3 * samples_per_segment:\n",
    "            LC = data[2 * samples_per_segment:3 * samples_per_segment, :]\n",
    "        if data.shape[0] >= 4 * samples_per_segment:\n",
    "            RC = data[3 * samples_per_segment:4 * samples_per_segment, :]\n",
    "\n",
    "        # Apply conditions based on cohort table\n",
    "        if cohort_row['LC'].values[0] == 'DEC':\n",
    "            # Assign 'DEC' to LC and 'NDEC' to RC\n",
    "            DEC = LC\n",
    "            NDEC = RC\n",
    "        elif cohort_row['RC'].values[0] == 'DEC':\n",
    "            # Assign 'DEC' to RC and 'NDEC' to LC\n",
    "            DEC = RC\n",
    "            NDEC = LC\n",
    "        else:\n",
    "            # If neither LC nor RC is 'DEC', assign NDEC accordingly\n",
    "            NDEC = LC\n",
    "            # Optionally handle cases where DEC is not specified\n",
    "            DEC = RC  # Or set DEC to zeros if appropriate\n",
    "\n",
    "        # Store the segmented data and 'LinesDifference' in the results dictionary\n",
    "        segmented_data[key] = {\n",
    "            'header': hdr,\n",
    "            'EC': EC,\n",
    "            'EO': EO,\n",
    "            'DEC': DEC,\n",
    "            'NDEC': NDEC,\n",
    "            'LinesDifference': cohort_row['LinesDifference'].values[0]\n",
    "        }\n",
    "\n",
    "    return segmented_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data = segment_eeg_data_new(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def prepare_time_series_by_section(segmented_data, cohort_table):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame suitable for tsfresh from segmented EEG data for all sections (EC, EO, DEC, NDEC).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    segmented_data : dict\n",
    "        The dictionary containing segmented EEG data for each participant.\n",
    "    cohort_table : pd.DataFrame\n",
    "        DataFrame containing cohort information (including labels for Amblyopia/Control).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame, pd.Series\n",
    "        A DataFrame where each row represents a time-series sample with columns 'id', 'time', 'O1', 'Oz', 'O2',\n",
    "        and a Series with group labels indexed by 'id'.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Loop through each participant's data\n",
    "    for key, value in segmented_data.items():\n",
    "        # Find the matching cohort row\n",
    "        cohort_row = cohort_table[cohort_table['Cohort'] == key]\n",
    "        if cohort_row.empty:\n",
    "            continue\n",
    "\n",
    "        # Assign label based on the first letter of the 'Cohort' column (Amblyopia = 1, Control = 0)\n",
    "        label = 1 if key.startswith('A') else 0\n",
    "\n",
    "        # Get channel names; default to ['O1', 'Oz', 'O2'] if not available\n",
    "        channels = value.get('channels', ['O1', 'Oz', 'O2'])\n",
    "\n",
    "        # For each section (EC, EO, DEC, NDEC)\n",
    "        for section in ['EC', 'EO', 'DEC', 'NDEC']:\n",
    "            section_data = value[section]  # Shape: (n_samples, n_channels)\n",
    "\n",
    "            # Create a DataFrame for this section\n",
    "            n_samples = section_data.shape[0]\n",
    "            df = pd.DataFrame({\n",
    "                'id': f\"{key}_{section}\",\n",
    "                'time': np.arange(n_samples)\n",
    "            })\n",
    "\n",
    "            # Add each channel's data as a column\n",
    "            for idx, channel_name in enumerate(channels):\n",
    "                df[channel_name] = section_data[:, idx]\n",
    "\n",
    "            # Append to data list\n",
    "            data_list.append(df)\n",
    "\n",
    "            # Append label for this 'id' (participant_section)\n",
    "            labels_list.append({'id': f\"{key}_{section}\", 'label': label})\n",
    "\n",
    "    # Concatenate all data into a single DataFrame\n",
    "    time_series_df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "    # Create a labels DataFrame and convert to a Series indexed by 'id'\n",
    "    labels_df = pd.DataFrame(labels_list).drop_duplicates(subset='id')\n",
    "    labels_series = labels_df.set_index('id')['label']\n",
    "\n",
    "    # Return the time-series data and corresponding labels\n",
    "    return time_series_df, labels_series\n",
    "\n",
    "# Load your cohort table (must include 'Cohort' column)\n",
    "cohort_table = pd.read_excel('Cohort.xlsx')\n",
    "\n",
    "# Prepare the time series DataFrame and labels\n",
    "time_series_df, labels = prepare_time_series_by_section(segmented_data, cohort_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>O1</th>\n",
       "      <th>Oz</th>\n",
       "      <th>O2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1_EC</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1_EC</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1_EC</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1_EC</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1_EC</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064955</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>20475</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064956</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>20476</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064957</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>20477</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064958</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>20478</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064959</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>20479</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1064960 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id   time        O1        Oz        O2\n",
       "0          A1_EC      0  0.000016  0.000016  0.000013\n",
       "1          A1_EC      1  0.000014  0.000015  0.000012\n",
       "2          A1_EC      2  0.000013  0.000013  0.000011\n",
       "3          A1_EC      3  0.000011  0.000011  0.000011\n",
       "4          A1_EC      4  0.000010  0.000010  0.000010\n",
       "...          ...    ...       ...       ...       ...\n",
       "1064955  C1_NDEC  20475 -0.000003 -0.000004 -0.000004\n",
       "1064956  C1_NDEC  20476 -0.000003 -0.000003 -0.000004\n",
       "1064957  C1_NDEC  20477 -0.000003 -0.000002 -0.000003\n",
       "1064958  C1_NDEC  20478 -0.000003 -0.000002 -0.000003\n",
       "1064959  C1_NDEC  20479 -0.000003 -0.000002 -0.000002\n",
       "\n",
       "[1064960 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save time_series_df as CSV\n",
    "time_series_df.to_csv('time_series_df_full.csv', index=False)\n",
    "\n",
    "# Save labels as CSV\n",
    "labels.to_csv('labels_full.csv', index=False, header=True)\n",
    "\n",
    "# Optionally, save labels as Pickle (preserves Python object types)\n",
    "# labels.to_pickle('labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read time_series_df from CSV\n",
    "# time_series_df = pd.read_csv('time_series_df_full.csv')\n",
    "\n",
    "# # Read labels from CSV\n",
    "# labels = pd.read_csv('labels_full.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.index = time_series_df['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 15/15 [00:04<00:00,  3.08it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:03<00:00,  4.21it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:03<00:00,  4.38it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:03<00:00,  4.44it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:06<00:00,  2.27it/s]\n",
      "Feature Extraction: 100%|██████████| 6/6 [00:05<00:00,  1.05it/s]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [ 3 13 23] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         6\n",
      "           1       1.00      0.40      0.57        10\n",
      "\n",
      "    accuracy                           0.62        16\n",
      "   macro avg       0.75      0.70      0.62        16\n",
      "weighted avg       0.81      0.62      0.61        16\n",
      "\n",
      "                  Feature  Importance\n",
      "4             O1__maximum    0.207673\n",
      "8    O2__absolute_maximum    0.140004\n",
      "7             O2__maximum    0.126442\n",
      "9             O2__minimum    0.125662\n",
      "1  O1__standard_deviation    0.092274\n",
      "5    O1__absolute_maximum    0.088193\n",
      "6             O1__minimum    0.083645\n",
      "3    O1__root_mean_square    0.077596\n",
      "0              O1__median    0.058510\n",
      "2            O1__variance    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "\n",
    "# Define the function to process data in chunks using ComprehensiveFCParameters\n",
    "def process_in_chunks(time_series_df, N):\n",
    "    # Get unique IDs\n",
    "    unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the IDs in the current chunk\n",
    "        chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk using ComprehensiveFCParameters\n",
    "        extracted_features_chunk = extract_features(\n",
    "            chunk_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=MinimalFCParameters()\n",
    "        )\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features_chunk)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_df, extracted_features_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Set the chunk size N (adjust based on your memory constraints)\n",
    "N = 10  # Smaller chunk size to manage memory usage\n",
    "\n",
    "# Extract features using the process_in_chunks function\n",
    "extracted_features = process_in_chunks(time_series_df, N)\n",
    "\n",
    "# Drop any columns with NaN or infinite values\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Ensure that the labels are aligned with the extracted features\n",
    "# Assuming 'labels' is a Series with 'id' as the index\n",
    "labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_features_clean,\n",
    "    labels_aligned,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Select the most important features using ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=10)  # Adjust 'k' as needed\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best estimator from GridSearchCV to predict and evaluate the model\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Identify and display the top selected features with importance\n",
    "selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "important_features = pd.DataFrame({\n",
    "    'Feature': selected_feature_names,\n",
    "    'Importance': best_clf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(important_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Best Parameters Found by GridSearchCV\n",
    "plaintext\n",
    "Copy code\n",
    "Best parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
    "Explanation:\n",
    "\n",
    "max_depth: None: This means there's no limit to how deep each tree in the forest can grow. The nodes will expand until all leaves are pure or until all leaves contain fewer samples than min_samples_split.\n",
    "\n",
    "min_samples_split: 2: This is the minimum number of samples required to split an internal node. A value of 2 is the default and allows the tree to grow as much as possible.\n",
    "\n",
    "n_estimators: 100: The number of trees in the forest is 100. More trees can lead to better performance but also increase computation time.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The grid search determined that the default parameters are optimal within the range you provided. Essentially, the model performs best without restrictions on tree depth and with the default settings for splitting and the number of trees.\n",
    "\n",
    "2. Classification Report\n",
    "plaintext\n",
    "Copy code\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      1.00      0.67         6\n",
    "           1       1.00      0.40      0.57        10\n",
    "\n",
    "    accuracy                           0.62        16\n",
    "   macro avg       0.75      0.70      0.62        16\n",
    "weighted avg       0.81      0.62      0.61        16\n",
    "Metrics Explanation:\n",
    "\n",
    "Support: The number of occurrences of each class in the test set.\n",
    "\n",
    "Class 0: 6 instances.\n",
    "Class 1: 10 instances.\n",
    "Precision: The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "\n",
    "Class 0: 0.50 (50% of the instances predicted as class 0 are actually class 0).\n",
    "Class 1: 1.00 (100% of the instances predicted as class 1 are actually class 1).\n",
    "Recall: The ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "Class 0: 1.00 (100% of actual class 0 instances are correctly identified).\n",
    "Class 1: 0.40 (Only 40% of actual class 1 instances are correctly identified).\n",
    "F1-score: The harmonic mean of precision and recall.\n",
    "\n",
    "Class 0: 0.67.\n",
    "Class 1: 0.57.\n",
    "Accuracy: Overall, 62% of the test set instances are correctly classified.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Class 0 (Control Group or Non-Amblyopia):\n",
    "\n",
    "High Recall (1.00): The model correctly identified all instances of class 0.\n",
    "Low Precision (0.50): Half of the instances predicted as class 0 are actually from class 1 (false positives).\n",
    "Class 1 (Amblyopia Group):\n",
    "\n",
    "High Precision (1.00): All instances predicted as class 1 are correctly from class 1.\n",
    "Low Recall (0.40): The model failed to identify 60% of actual class 1 instances (false negatives).\n",
    "Overall Performance:\n",
    "\n",
    "The model is better at identifying class 0 but struggles to correctly identify all instances of class 1.\n",
    "Accuracy is 62%, which may not be satisfactory depending on the context.\n",
    "Macro Average:\n",
    "Precision: 0.75.\n",
    "Recall: 0.70.\n",
    "F1-score: 0.62.\n",
    "Possible Reasons for the Performance:\n",
    "\n",
    "Class Imbalance: Although the classes are relatively balanced (6 vs. 10), the model may still be biased towards class 0.\n",
    "Small Dataset: With only 16 instances in the test set, the model's performance metrics may not be stable or representative.\n",
    "Overfitting: The model may have overfitted to the training data, especially if the training set is small or if the model is too complex.\n",
    "Feature Selection: Limiting to 10 features may have excluded important predictors.\n",
    "3. Important Features and Their Importances\n",
    "plaintext\n",
    "Copy code\n",
    "                      Feature  Importance\n",
    "4             O1__maximum    0.207673\n",
    "8    O2__absolute_maximum    0.140004\n",
    "7             O2__maximum    0.126442\n",
    "9             O2__minimum    0.125662\n",
    "1  O1__standard_deviation    0.092274\n",
    "5    O1__absolute_maximum    0.088193\n",
    "6             O1__minimum    0.083645\n",
    "3    O1__root_mean_square    0.077596\n",
    "0              O1__median    0.058510\n",
    "2            O1__variance    0.000000\n",
    "Feature Descriptions:\n",
    "\n",
    "Channel O1:\n",
    "\n",
    "O1__maximum: The maximum value of the EEG signal in the O1 channel.\n",
    "O1__absolute_maximum: The largest absolute value in the O1 channel.\n",
    "O1__minimum: The minimum value in the O1 channel.\n",
    "O1__standard_deviation: The standard deviation of the O1 signal.\n",
    "O1__root_mean_square: The RMS value of the O1 signal.\n",
    "O1__median: The median value of the O1 signal.\n",
    "O1__variance: The variance of the O1 signal.\n",
    "Channel O2:\n",
    "\n",
    "O2__absolute_maximum: The largest absolute value in the O2 channel.\n",
    "O2__maximum: The maximum value of the EEG signal in the O2 channel.\n",
    "O2__minimum: The minimum value in the O2 channel.\n",
    "Feature Importances:\n",
    "\n",
    "The importance values indicate the relative contribution of each feature to the model's decision-making.\n",
    "\n",
    "Top Features:\n",
    "\n",
    "O1__maximum (0.2077): Most significant feature.\n",
    "O2__absolute_maximum (0.1400).\n",
    "O2__maximum (0.1264).\n",
    "O2__minimum (0.1257).\n",
    "Zero Importance Feature:\n",
    "\n",
    "O1__variance (0.0000): This feature did not contribute to the model's predictions.\n",
    "Interpretation:\n",
    "\n",
    "The model heavily relies on maximum and minimum amplitude values from channels O1 and O2.\n",
    "Features like standard deviation and root mean square also play a role but are less significant.\n",
    "The variance of the O1 signal didn't contribute, possibly due to redundancy with other features or lack of discriminative power.\n",
    "4. Overall Analysis and Recommendations\n",
    "Understanding the Model's Behavior:\n",
    "\n",
    "High Recall for Class 0: The model correctly identifies all control group instances but at the cost of misclassifying many amblyopia instances as controls.\n",
    "Low Recall for Class 1: The model misses 60% of the amblyopia cases, which is critical if the goal is to detect amblyopia.\n",
    "Feature Dependence: The model's reliance on extreme values (maximum and minimum) may make it sensitive to noise or outliers in the data.\n",
    "Possible Issues:\n",
    "\n",
    "Data Quality: EEG data can be noisy. Extreme values may be influenced by artifacts rather than true neural activity.\n",
    "Overfitting to Noise: Focusing on maximum and minimum values might cause the model to capture noise rather than meaningful patterns.\n",
    "Small Sample Size: With a limited number of samples, especially in the test set, performance metrics may not be reliable.\n",
    "Feature Selection Limitations: Selecting only 10 features may not capture the complexity needed to differentiate between classes.\n",
    "Recommendations:\n",
    "\n",
    "Increase Dataset Size:\n",
    "\n",
    "Collect more EEG recordings to provide the model with more examples to learn from.\n",
    "Enhance Feature Extraction:\n",
    "\n",
    "Use more comprehensive feature extraction methods, possibly including frequency-domain features (e.g., power spectral density).\n",
    "Consider time-frequency analysis (e.g., wavelet transforms) to capture transient events.\n",
    "Feature Selection Strategy:\n",
    "\n",
    "Instead of selecting a fixed number of features (k), consider using all features or use techniques like recursive feature elimination (RFE) to find the optimal feature subset.\n",
    "Evaluate feature importance using different criteria, such as mutual information.\n",
    "Address Class Imbalance:\n",
    "\n",
    "Although the class distribution isn't severely imbalanced, using techniques like SMOTE (Synthetic Minority Over-sampling Technique) can help the model learn better representations of the minority class.\n",
    "Ensure that the train-test split maintains class distribution (stratified sampling).\n",
    "Model Tuning and Validation:\n",
    "\n",
    "Experiment with different models, such as Gradient Boosting Machines, Support Vector Machines, or Neural Networks.\n",
    "Use cross-validation to obtain a more reliable estimate of the model's performance.\n",
    "Adjust Evaluation Metrics:\n",
    "\n",
    "Since missing amblyopia cases is more critical, consider optimizing for recall on class 1.\n",
    "Use metrics like ROC AUC or Precision-Recall curves to get a better understanding of model performance.\n",
    "Data Preprocessing:\n",
    "\n",
    "Apply signal processing techniques to reduce noise, such as filtering or artifact rejection.\n",
    "Normalize or standardize the features to reduce the impact of scale differences.\n",
    "Investigate Feature Importances:\n",
    "\n",
    "Analyze why certain features are more important.\n",
    "Consider if these features make sense from a neuroscientific perspective.\n",
    "Conclusion\n",
    "Model Limitations: The current model doesn't perform adequately, especially in detecting the amblyopia class, which is critical for your application.\n",
    "Next Steps: Implement the recommendations to improve data quality, feature richness, and model robustness.\n",
    "Continuous Evaluation: As you make changes, continue to evaluate the model using appropriate metrics and validation strategies.\n",
    "Remember: Machine learning in healthcare and biomedical applications often requires careful consideration of data quality, feature engineering, and ethical implications of false negatives and false positives. It's crucial to ensure that the model is reliable and performs well on the aspects that matter most for the intended use case.\n",
    "\n",
    "Let me know if you have any questions or need further clarification on any of these points!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 15/15 [00:04<00:00,  3.71it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:03<00:00,  4.67it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:02<00:00,  5.26it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:02<00:00,  5.29it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:02<00:00,  5.39it/s]\n",
      "Feature Extraction: 100%|██████████| 6/6 [00:02<00:00,  2.16it/s]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [ 3 13 23] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.86      0.75         7\n",
      "           1       0.86      0.67      0.75         9\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.76      0.76      0.75        16\n",
      "weighted avg       0.77      0.75      0.75        16\n",
      "\n",
      "                  Feature  Importance\n",
      "8    O2__absolute_maximum    0.174592\n",
      "9             O2__minimum    0.151715\n",
      "3             O1__maximum    0.147910\n",
      "4    O1__absolute_maximum    0.130825\n",
      "7             O2__maximum    0.105793\n",
      "5  O2__standard_deviation    0.080456\n",
      "6    O2__root_mean_square    0.080394\n",
      "0  O1__standard_deviation    0.070301\n",
      "2    O1__root_mean_square    0.058013\n",
      "1            O1__variance    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# Define the function to process data in chunks using ComprehensiveFCParameters\n",
    "def process_in_chunks(time_series_df, N):\n",
    "    # Get unique IDs\n",
    "    unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the IDs in the current chunk\n",
    "        chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk using ComprehensiveFCParameters\n",
    "        extracted_features_chunk = extract_features(\n",
    "            chunk_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=MinimalFCParameters(),  # Use ComprehensiveFCParameters() for more features\n",
    "            n_jobs=4,  # Adjust based on your CPU cores\n",
    "            # Since data is in wide format, we do not need to specify column_kind and column_value\n",
    "        )\n",
    "        \n",
    "        # Impute missing values in the extracted features\n",
    "        impute(extracted_features_chunk)\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features_chunk)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_df, extracted_features_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Set the chunk size N (adjust based on your memory constraints)\n",
    "N = 10  # Smaller chunk size to manage memory usage\n",
    "\n",
    "# Extract features using the process_in_chunks function\n",
    "extracted_features = process_in_chunks(time_series_df, N)\n",
    "\n",
    "# Drop any columns with NaN or infinite values\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Ensure that the labels are aligned with the extracted features\n",
    "# Assuming 'labels' is a Series with 'id' as the index\n",
    "labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_features_clean,\n",
    "    labels_aligned,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=labels_aligned  # Ensure stratified sampling\n",
    ")\n",
    "\n",
    "# Select the most important features using ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=10)  # Adjust 'k' as needed\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best estimator from GridSearchCV to predict and evaluate the model\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Identify and display the top selected features with importance\n",
    "important_features = pd.DataFrame({\n",
    "    'Feature': selected_feature_names,\n",
    "    'Importance': best_clf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained classifier\n",
    "joblib.dump(best_clf, 'trained_random_forest.pkl')\n",
    "\n",
    "# Save the feature selector\n",
    "joblib.dump(selector, 'feature_selector.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import classification_report\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import gc\n",
    "# from tsfresh import extract_features\n",
    "# from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "# from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# # Define the function to process data in chunks using ComprehensiveFCParameters\n",
    "# def process_in_chunks(time_series_df, N, n_jobs):\n",
    "#     # Get unique IDs\n",
    "#     unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "#     # Split the unique IDs into chunks of size N\n",
    "#     chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "#     # Initialize an empty list to store the results\n",
    "#     results = []\n",
    "    \n",
    "#     # Process each chunk\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         print(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "#         # Filter the DataFrame to include only the IDs in the current chunk\n",
    "#         chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "#         # Extract features for the current chunk using ComprehensiveFCParameters\n",
    "#         extracted_features_chunk = extract_features(\n",
    "#             chunk_df,\n",
    "#             column_id='id',\n",
    "#             column_sort='time',\n",
    "#             default_fc_parameters=ComprehensiveFCParameters(),\n",
    "#             n_jobs=n_jobs,  # Adjust based on your CPU cores and memory\n",
    "#             # Since data is in wide format, we do not need to specify column_kind and column_value\n",
    "#         )\n",
    "        \n",
    "#         # Impute missing values in the extracted features\n",
    "#         impute(extracted_features_chunk)\n",
    "        \n",
    "#         # Append the extracted features to the results list\n",
    "#         results.append(extracted_features_chunk)\n",
    "        \n",
    "#         # Clear memory\n",
    "#         del chunk_df, extracted_features_chunk\n",
    "#         gc.collect()\n",
    "    \n",
    "#     # Concatenate all the results into a single DataFrame\n",
    "#     final_result = pd.concat(results)\n",
    "    \n",
    "#     return final_result\n",
    "\n",
    "# # Set the chunk size N (adjust based on your memory constraints)\n",
    "# N = 5  # Smaller chunk size to manage memory usage\n",
    "\n",
    "# # Set the number of jobs for parallel processing\n",
    "# n_jobs = 2  # Adjust based on your CPU cores and memory capacity\n",
    "\n",
    "# # Extract features using the process_in_chunks function\n",
    "# extracted_features = process_in_chunks(time_series_df, N, n_jobs)\n",
    "\n",
    "# # Drop any columns with NaN or infinite values\n",
    "# extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# # Ensure that the labels are aligned with the extracted features\n",
    "# # Assuming 'labels' is a Series with 'id' as the index\n",
    "# labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     extracted_features_clean,\n",
    "#     labels_aligned,\n",
    "#     test_size=0.3,\n",
    "#     random_state=42,\n",
    "#     stratify=labels_aligned  # Ensure stratified sampling\n",
    "# )\n",
    "\n",
    "# # Select the most important features using ANOVA F-test\n",
    "# selector = SelectKBest(f_classif, k=20)  # Adjust 'k' as needed\n",
    "# X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "# X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# # Get the names of the selected features\n",
    "# selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "\n",
    "# # Train a Random Forest Classifier\n",
    "# clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Define parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100],\n",
    "#     'max_depth': [None],\n",
    "#     'min_samples_split': [2],\n",
    "# }\n",
    "\n",
    "# # Perform grid search to find the best parameters\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "# grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# # Print the best parameters found by GridSearchCV\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# # Use the best estimator from GridSearchCV to predict and evaluate the model\n",
    "# best_clf = grid_search.best_estimator_\n",
    "# y_pred = best_clf.predict(X_test_selected)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Identify and display the top selected features with importance\n",
    "# important_features = pd.DataFrame({\n",
    "#     'Feature': selected_feature_names,\n",
    "#     'Importance': best_clf.feature_importances_\n",
    "# }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# print(important_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
