{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mne scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read time_series_df from CSV\n",
    "time_series_df = pd.read_csv('time_series_df.csv')\n",
    "\n",
    "# Read labels from CSV\n",
    "labels = pd.read_csv('labels.csv', squeeze=True)  # Use squeeze=True to load it as a Series if it's a single column\n",
    "\n",
    "# Optionally, read labels from Pickle (preserves Python object types)\n",
    "# labels = pd.read_pickle('labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.index = time_series_df['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, ComprehensiveFCParameters\n",
    "\n",
    "# Extract features using tsfresh with minimal settings\n",
    "extracted_features = extract_features(time_series_df, column_id='id', column_sort='time',\n",
    "                                      default_fc_parameters=MinimalFCParameters())\n",
    "\n",
    "# Drop any columns with NaN or infinite values\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(extracted_features_clean, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Select the most important features using ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=10)  # Adjust 'k' to select the top k important features\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Classifier to identify the most important features\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV (for Random Forest)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters for Random Forest\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best estimator from GridSearchCV to predict and evaluate the model\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Identify and display the top selected features with importance\n",
    "selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "important_features = pd.DataFrame({\n",
    "    'Feature': selected_feature_names,\n",
    "    'Importance': best_clf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_chunks(time_series_df, N):\n",
    "    # Get unique trial IDs\n",
    "    unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the trials in the current chunk\n",
    "        chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk\n",
    "        extracted_features = extract_features(chunk_df, column_id='id', column_sort='time', default_fc_parameters=ComprehensiveFCParameters())\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features)\n",
    "        break\n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = process_in_chunks(time_series_df, N=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Classifier Performance:\n",
    "The Random Forest model's classification performance is summarized in the precision, recall, f1-score, and support columns for both classes:\n",
    "\n",
    "- Class 0 (Control): The model predicted this class with a precision of 0.12, a recall of 0.17, and an F1-score of 0.14.\n",
    "- Class 1 (Amblyopia): The model predicted this class with a precision of 0.38, a recall of 0.30, and an F1-score of 0.33.\n",
    "- Overall Accuracy: The model's overall accuracy is 0.25 (25%), which indicates that the model didn't perform well in distinguishing between Amblyopia and Control participants.\n",
    "\n",
    "Key Metrics:\n",
    "\n",
    "Precision: Measures how many of the predicted positive results are true positives.\n",
    "- For class 0 (Control), only 12% of the instances predicted as Control were correct.\n",
    "- For class 1 (Amblyopia), 38% of the instances predicted as Amblyopia were correct.\n",
    "\n",
    "Recall: Measures how many actual positive instances were correctly predicted.\n",
    "- For class 0, 17% of the actual Control instances were correctly identified.\n",
    "- For class 1, 30% of the actual Amblyopia instances were correctly identified.\n",
    "\n",
    "F1-Score: A harmonic mean of precision and recall, giving a better sense of the balance between the two metrics. In both classes, the F1-scores are relatively low, especially for Control.\n",
    "\n",
    "This low performance (accuracy 25%) suggests that the model struggled to differentiate between the two groups based on the extracted features. This could be due to various reasons, such as insufficient or irrelevant features, a small dataset, or an imbalance between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Importance:\n",
    "This table ranks the extracted features by their importance, as determined by the Random Forest classifier:\n",
    "\n",
    "Feature\tImportance\n",
    "- 8    value__absolute_maximum    0.214460\n",
    "- 7             value__maximum    0.168308\n",
    "- 9             value__minimum    0.159271\n",
    "- 4  value__standard_deviation    0.146875\n",
    "- 6    value__root_mean_square    0.134352\n",
    "- 0          value__sum_values    0.128472\n",
    "- 1              value__median    0.048262\n",
    "- 2                value__mean    0.000000\n",
    "- 3              value__length    0.000000\n",
    "- 5            value__variance    0.000000\n",
    "\n",
    "Most Important Features:\n",
    "\n",
    "- value__absolute_maximum: This feature, which represents the absolute maximum value in the time series, was the most important in distinguishing between Amblyopia and Control participants (importance = 0.214460).\n",
    "\n",
    "Observations:\n",
    "\n",
    "- Feature Importance Distribution: The features' importance values are skewed, with the top 6 features contributing most to the model's predictions, while others (such as mean, length, and variance) contributed very little or nothing.\n",
    "\n",
    "Interpretation:\n",
    "- Low Accuracy: The classifier struggled to differentiate between the Amblyopia and Control groups, possibly because the extracted features don't capture the necessary information to distinguish between these groups, or the dataset might be too small or imbalanced.\n",
    "- Feature Importance: Features like absolute_maximum, maximum, and minimum seem to provide the most information for distinguishing between Amblyopia and Control participants. These might represent extreme fluctuations or peak characteristics in the EEG signals.\n",
    "\n",
    "Next Steps:\n",
    "- Feature Engineering: Consider extracting additional or more complex features using tsfresh or other methods to capture more meaningful aspects of the EEG data.\n",
    "- Balanced Dataset: Ensure that the dataset is balanced between Amblyopia and Control groups to avoid skewed performance metrics.\n",
    "- Cross-Validation: Use cross-validation to get a more robust estimate of model performance.\n",
    "- Other Models: Try different classifiers (e.g., Support Vector Machines or Gradient Boosting) to see if they perform better.\n",
    "- Hyperparameter Tuning: Tuning the Random Forest classifier (e.g., adjusting the number of trees or maximum depth) could potentially improve the model's accuracy.\n",
    "Let me know if you need further insights or improvements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m important_features \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimportant_features.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# import pandas as pd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(pd.__version__)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# !pip install --upgrade pandas\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[0;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\numpy_pickle.py:415\u001b[0m, in \u001b[0;36mNumpyUnpickler.load_build\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array_wrapper, NDArrayWrapper):\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompat_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mappend(\u001b[43marray_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\numpy_pickle.py:252\u001b[0m, in \u001b[0;36mNumpyArrayWrapper.read\u001b[1;34m(self, unpickler)\u001b[0m\n\u001b[0;32m    250\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_mmap(unpickler)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 252\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43munpickler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Manage array subclass case\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(array, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__array_prepare__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubclass \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (unpickler\u001b[38;5;241m.\u001b[39mnp\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m    257\u001b[0m                           unpickler\u001b[38;5;241m.\u001b[39mnp\u001b[38;5;241m.\u001b[39mmemmap)):\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# We need to reconstruct another subclass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\numpy_pickle.py:152\u001b[0m, in \u001b[0;36mNumpyArrayWrapper.read_array\u001b[1;34m(self, unpickler)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Now read the actual data.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     numpy_array_alignment_bytes \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msafe_get_numpy_array_alignment_bytes()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "important_features = joblib.load('important_features.joblib')\n",
    "\n",
    "# import pandas as pd\n",
    "# print(pd.__version__)\n",
    "# !pip install --upgrade pandas\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
