{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mne scipy\n",
    "#!pip install pandas numpy openpyxl\n",
    "#!pip install tsfresh\n",
    "#!pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import mne\n",
    "\n",
    "def process_all_eeg_data() -> dict:\n",
    "    \"\"\"\n",
    "    Process all .bdf EEG files in the current directory, applying filters and extracting data from\n",
    "    channels A15 (O1), A16 (Oz), and A17 (O2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing processed EEG data and header information for each file.\n",
    "    \"\"\"\n",
    "    # Get a list of all .bdf files in the current directory\n",
    "    files = [f for f in os.listdir('.') if f.endswith('.bdf')]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No BDF files found in the current directory\")\n",
    "    \n",
    "    # Initialize the results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Loop over each file\n",
    "    for filename in files:\n",
    "        full_file_path = os.path.join(os.getcwd(), filename)\n",
    "        \n",
    "        # Read the raw EEG data using MNE\n",
    "        raw = mne.io.read_raw_bdf(full_file_path, preload=True)\n",
    "        hdr = raw.info\n",
    "        \n",
    "        # Select data from channels A15 (O1), A16 (Oz), and A17 (O2)\n",
    "        channels_select = ['A15', 'A16', 'A17']\n",
    "        missing_channels = [ch for ch in channels_select if ch not in hdr['ch_names']]\n",
    "        if missing_channels:\n",
    "            raise ValueError(f\"Selected channels {missing_channels} not found in the data\")\n",
    "        \n",
    "        channel_indices = [hdr['ch_names'].index(ch) for ch in channels_select]\n",
    "        EEG_data = raw.get_data(picks=channel_indices).T  # Shape: (n_samples, n_channels)\n",
    "        \n",
    "        # Filter EEG Data\n",
    "        Fs = hdr['sfreq']  # Sampling frequency\n",
    "        \n",
    "        # Bandpass filter parameters (2 to 80 Hz)\n",
    "        Fc_BP = [2, 80]  # Bandpass frequency range\n",
    "        Wn_BP = [f / (Fs / 2) for f in Fc_BP]  # Normalize by Nyquist frequency\n",
    "        \n",
    "        # Create and apply bandpass filter (6th order zero-phase Butterworth IIR)\n",
    "        B_BP, A_BP = signal.butter(3, Wn_BP, btype='bandpass')\n",
    "        EEG_filtered_BP = signal.filtfilt(B_BP, A_BP, EEG_data, axis=0)\n",
    "        \n",
    "        # Band stop filter parameters (48 to 52 Hz)\n",
    "        Fc_BS = [48, 52]  # Band stop frequency range\n",
    "        Wn_BS = [f / (Fs / 2) for f in Fc_BS]  # Normalize by Nyquist frequency\n",
    "        \n",
    "        # Create and apply band stop filter (6th order zero-phase Butterworth IIR)\n",
    "        B_BS, A_BS = signal.butter(3, Wn_BS, btype='bandstop')\n",
    "        EEG_filtered = signal.filtfilt(B_BS, A_BS, EEG_filtered_BP, axis=0)\n",
    "        \n",
    "        # Extract prefix before underscore from the filename\n",
    "        underscore_index = filename.find('_')\n",
    "        if underscore_index == -1:\n",
    "            raise ValueError(f\"Filename format error, no underscore found in {filename}\")\n",
    "        key = filename[:underscore_index]\n",
    "        \n",
    "        # Store results in the dictionary\n",
    "        results[key] = {\n",
    "            'data': EEG_filtered,      # Filtered data for channels A15, A16, A17\n",
    "            'channels': channels_select,  # List of channel names\n",
    "            'header': hdr\n",
    "        }\n",
    "        \n",
    "        # Display a message indicating successful processing\n",
    "        print(f\"Data for file {filename} processed successfully\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A1_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 739327  =      0.000 ...   361.000 secs...\n",
      "Data for file A1_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A3_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 757759  =      0.000 ...   370.000 secs...\n",
      "Data for file A3_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A4_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 782335  =      0.000 ...   382.000 secs...\n",
      "Data for file A4_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A6_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A6_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A7_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A7_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A8_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A8_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A9_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A9_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C11_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C11_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C12_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C12_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C13_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C13_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C14_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C14_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C15_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C15_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C1_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C1_Alpha.bdf processed successfully\n"
     ]
    }
   ],
   "source": [
    "results = process_all_eeg_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def segment_eeg_data_new(results: dict, cohort_file: str = 'Cohort.xlsx') -> dict:\n",
    "    \"\"\"\n",
    "    Segments EEG data into predefined sections (EC, EO, LC, RC, DEC, NDEC) based on cohort information,\n",
    "    removing the first 2 seconds from each section.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        Dictionary containing the raw EEG data and header information for each key (participant).\n",
    "    cohort_file : str, optional\n",
    "        Path to the Excel file containing cohort information (default is 'Cohort.xlsx').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing segmented EEG data for each participant.\n",
    "    \"\"\"\n",
    "    # Read the cohort information from an Excel file\n",
    "    cohort_table = pd.read_excel(cohort_file)\n",
    "    # Segment Duration (in seconds)\n",
    "    segment_duration = 10  # Original segment duration in seconds\n",
    "    skip_duration = 2      # Duration to skip at the start of each segment (2 seconds)\n",
    "\n",
    "    # Initialize the segmented results dictionary\n",
    "    segmented_data = {}\n",
    "\n",
    "    # Iterate through each key in the results dictionary\n",
    "    for key, result in results.items():\n",
    "        data = result['data']  # Data shape: (n_samples, n_channels)\n",
    "        hdr = result['header']\n",
    "\n",
    "        # Find the matching row in the cohort table\n",
    "        cohort_row = cohort_table[cohort_table['Cohort'] == key]\n",
    "        \n",
    "        if cohort_row.empty:\n",
    "            raise ValueError(f\"Cohort information not found for {key}\")\n",
    "\n",
    "        # Define the sample rate and calculate sample counts\n",
    "        Fs = hdr['sfreq']  # Sampling frequency\n",
    "        samples_per_segment = int(segment_duration * Fs)\n",
    "        samples_to_skip = int(skip_duration * Fs)\n",
    "        effective_samples_per_segment = samples_per_segment - samples_to_skip\n",
    "        n_channels = data.shape[1]  # Number of channels (should be 3: O1, Oz, O2)\n",
    "\n",
    "        # Initialize segments with zeros\n",
    "        EC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        EO = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        LC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        RC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        DEC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        NDEC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "\n",
    "        # Fill segments with data if available, skipping the first 2 seconds\n",
    "        # EC segment\n",
    "        segment_start = 0\n",
    "        segment_end = samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            EC = data[segment_start + samples_to_skip : 0, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for EC segment in {key}\")\n",
    "\n",
    "        # EO segment\n",
    "        segment_start = samples_per_segment\n",
    "        segment_end = 2 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            EO = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for EO segment in {key}\")\n",
    "\n",
    "        # LC segment\n",
    "        segment_start = 2 * samples_per_segment\n",
    "        segment_end = 3 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            LC = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for LC segment in {key}\")\n",
    "\n",
    "        # RC segment\n",
    "        segment_start = 3 * samples_per_segment\n",
    "        segment_end = 4 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            RC = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for RC segment in {key}\")\n",
    "\n",
    "        # Apply conditions based on cohort table\n",
    "        if cohort_row['LC'].values[0] == 'DEC':\n",
    "            # Assign 'DEC' to LC and 'NDEC' to RC\n",
    "            DEC = LC\n",
    "            NDEC = RC\n",
    "        elif cohort_row['RC'].values[0] == 'DEC':\n",
    "            # Assign 'DEC' to RC and 'NDEC' to LC\n",
    "            DEC = RC\n",
    "            NDEC = LC\n",
    "        else:\n",
    "            # If neither LC nor RC is 'DEC', assign NDEC accordingly\n",
    "            NDEC = LC\n",
    "            # Optionally handle cases where DEC is not specified\n",
    "            DEC = RC  # Or set DEC to zeros if appropriate\n",
    "\n",
    "        # Store the segmented data and 'LinesDifference' in the results dictionary\n",
    "        segmented_data[key] = {\n",
    "            'header': hdr,\n",
    "            'EC': EC,\n",
    "            'EO': EO,\n",
    "            'DEC': DEC,\n",
    "            'NDEC': NDEC,\n",
    "            'LinesDifference': cohort_row['LinesDifference'].values[0]\n",
    "        }\n",
    "\n",
    "    return segmented_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data = segment_eeg_data_new(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def prepare_time_series_by_section(segmented_data, cohort_table):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame suitable for tsfresh from segmented EEG data for all sections (EC, EO, DEC, NDEC).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    segmented_data : dict\n",
    "        The dictionary containing segmented EEG data for each participant.\n",
    "    cohort_table : pd.DataFrame\n",
    "        DataFrame containing cohort information (including labels for Amblyopia/Control).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame, pd.Series\n",
    "        A DataFrame where each row represents a time-series sample with columns 'id', 'time', 'O1', 'Oz', 'O2',\n",
    "        and a Series with group labels indexed by 'id'.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Loop through each participant's data\n",
    "    for key, value in segmented_data.items():\n",
    "        # Find the matching cohort row\n",
    "        cohort_row = cohort_table[cohort_table['Cohort'] == key]\n",
    "        if cohort_row.empty:\n",
    "            continue\n",
    "\n",
    "        # Assign label based on the first letter of the 'Cohort' column (Amblyopia = 1, Control = 0)\n",
    "        label = 1 if key.startswith('A') else 0\n",
    "\n",
    "        # Get channel names; default to ['O1', 'Oz', 'O2'] if not available\n",
    "        channels = value.get('channels', ['O1', 'Oz', 'O2'])\n",
    "\n",
    "        # For each section (EC, EO, DEC, NDEC)\n",
    "        for section in ['EC', 'EO', 'DEC', 'NDEC']:\n",
    "            section_data = value[section]  # Shape: (n_samples, n_channels)\n",
    "\n",
    "            # Create a DataFrame for this section\n",
    "            n_samples = section_data.shape[0]\n",
    "            df = pd.DataFrame({\n",
    "                'id': f\"{key}_{section}\",\n",
    "                'time': np.arange(n_samples)\n",
    "            })\n",
    "\n",
    "            # Add each channel's data as a column\n",
    "            for idx, channel_name in enumerate(channels):\n",
    "                df[channel_name] = section_data[:, idx]\n",
    "\n",
    "            # Append to data list\n",
    "            data_list.append(df)\n",
    "\n",
    "            # Append label for this 'id' (participant_section)\n",
    "            labels_list.append({'id': f\"{key}_{section}\", 'label': label})\n",
    "\n",
    "    # Concatenate all data into a single DataFrame\n",
    "    time_series_df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "    # Create a labels DataFrame and convert to a Series indexed by 'id'\n",
    "    labels_df = pd.DataFrame(labels_list).drop_duplicates(subset='id')\n",
    "    labels_series = labels_df.set_index('id')['label']\n",
    "\n",
    "    # Return the time-series data and corresponding labels\n",
    "    return time_series_df, labels_series\n",
    "\n",
    "# Load your cohort table (must include 'Cohort' column)\n",
    "cohort_table = pd.read_excel('Cohort.xlsx')\n",
    "\n",
    "# Prepare the time series DataFrame and labels\n",
    "time_series_df, labels = prepare_time_series_by_section(segmented_data, cohort_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>O1</th>\n",
       "      <th>Oz</th>\n",
       "      <th>O2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1_EO</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1_EO</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1_EO</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1_EO</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1_EO</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638971</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>16379</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638972</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>16380</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638973</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>16381</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638974</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>16382</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638975</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>16383</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>638976 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id   time        O1        Oz        O2\n",
       "0         A1_EO      0 -0.000013 -0.000026 -0.000036\n",
       "1         A1_EO      1 -0.000016 -0.000028 -0.000038\n",
       "2         A1_EO      2 -0.000019 -0.000031 -0.000040\n",
       "3         A1_EO      3 -0.000022 -0.000034 -0.000042\n",
       "4         A1_EO      4 -0.000025 -0.000036 -0.000043\n",
       "...         ...    ...       ...       ...       ...\n",
       "638971  C1_NDEC  16379 -0.000003 -0.000004 -0.000004\n",
       "638972  C1_NDEC  16380 -0.000003 -0.000003 -0.000004\n",
       "638973  C1_NDEC  16381 -0.000003 -0.000002 -0.000003\n",
       "638974  C1_NDEC  16382 -0.000003 -0.000002 -0.000003\n",
       "638975  C1_NDEC  16383 -0.000003 -0.000002 -0.000002\n",
       "\n",
       "[638976 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save time_series_df as CSV\n",
    "time_series_df.to_csv('time_series_df_full.csv', index=False)\n",
    "\n",
    "# Save labels as CSV\n",
    "labels.to_csv('labels_full.csv', index=False, header=True)\n",
    "\n",
    "# Optionally, save labels as Pickle (preserves Python object types)\n",
    "# labels.to_pickle('labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read time_series_df from CSV\n",
    "# time_series_df = pd.read_csv('time_series_df_full.csv')\n",
    "\n",
    "# # Read labels from CSV\n",
    "# labels = pd.read_csv('labels_full.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 52 elements, new values have 39 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m \u001b[38;5;241m=\u001b[39m time_series_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[1;32mc:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:6313\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   6312\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[1;32m-> 6313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m   6315\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:814\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[1;32m--> 814\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[1;32mc:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\base.py:98\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 52 elements, new values have 39 elements"
     ]
    }
   ],
   "source": [
    "labels.index = time_series_df['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 15/15 [00:02<00:00,  7.30it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:01<00:00,  7.94it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:01<00:00,  8.34it/s]\n",
      "Feature Extraction: 100%|██████████| 14/14 [00:01<00:00,  7.47it/s]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [ 3 13 23] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71         6\n",
      "           1       0.75      0.50      0.60         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.69      0.67      0.66        12\n",
      "weighted avg       0.69      0.67      0.66        12\n",
      "\n",
      "                  Feature  Importance\n",
      "9             O2__minimum    0.160322\n",
      "0             O1__maximum    0.143272\n",
      "2          O2__sum_values    0.136311\n",
      "4  O2__standard_deviation    0.127567\n",
      "6    O2__root_mean_square    0.124693\n",
      "1    O1__absolute_maximum    0.104424\n",
      "7             O2__maximum    0.103111\n",
      "8    O2__absolute_maximum    0.100300\n",
      "3                O2__mean    0.000000\n",
      "5            O2__variance    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "\n",
    "# Define the function to process data in chunks using ComprehensiveFCParameters\n",
    "def process_in_chunks(time_series_df, N):\n",
    "    # Get unique IDs\n",
    "    unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the IDs in the current chunk\n",
    "        chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk using ComprehensiveFCParameters\n",
    "        extracted_features_chunk = extract_features(\n",
    "            chunk_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=MinimalFCParameters()\n",
    "        )\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features_chunk)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_df, extracted_features_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Set the chunk size N (adjust based on your memory constraints)\n",
    "N = 10  # Smaller chunk size to manage memory usage\n",
    "\n",
    "# Extract features using the process_in_chunks function\n",
    "extracted_features = process_in_chunks(time_series_df, N)\n",
    "\n",
    "# Drop any columns with NaN or infinite values\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Ensure that the labels are aligned with the extracted features\n",
    "# Assuming 'labels' is a Series with 'id' as the index\n",
    "labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_features_clean,\n",
    "    labels_aligned,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Select the most important features using ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=10)  # Adjust 'k' as needed\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best estimator from GridSearchCV to predict and evaluate the model\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Identify and display the top selected features with importance\n",
    "selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "important_features = pd.DataFrame({\n",
    "    'Feature': selected_feature_names,\n",
    "    'Importance': best_clf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(important_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Best Parameters Found by GridSearchCV\n",
    "plaintext\n",
    "Copy code\n",
    "Best parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
    "Explanation:\n",
    "\n",
    "max_depth: None: This means there's no limit to how deep each tree in the forest can grow. The nodes will expand until all leaves are pure or until all leaves contain fewer samples than min_samples_split.\n",
    "\n",
    "min_samples_split: 2: This is the minimum number of samples required to split an internal node. A value of 2 is the default and allows the tree to grow as much as possible.\n",
    "\n",
    "n_estimators: 100: The number of trees in the forest is 100. More trees can lead to better performance but also increase computation time.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The grid search determined that the default parameters are optimal within the range you provided. Essentially, the model performs best without restrictions on tree depth and with the default settings for splitting and the number of trees.\n",
    "\n",
    "2. Classification Report\n",
    "plaintext\n",
    "Copy code\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      1.00      0.67         6\n",
    "           1       1.00      0.40      0.57        10\n",
    "\n",
    "    accuracy                           0.62        16\n",
    "   macro avg       0.75      0.70      0.62        16\n",
    "weighted avg       0.81      0.62      0.61        16\n",
    "Metrics Explanation:\n",
    "\n",
    "Support: The number of occurrences of each class in the test set.\n",
    "\n",
    "Class 0: 6 instances.\n",
    "Class 1: 10 instances.\n",
    "Precision: The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "\n",
    "Class 0: 0.50 (50% of the instances predicted as class 0 are actually class 0).\n",
    "Class 1: 1.00 (100% of the instances predicted as class 1 are actually class 1).\n",
    "Recall: The ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "Class 0: 1.00 (100% of actual class 0 instances are correctly identified).\n",
    "Class 1: 0.40 (Only 40% of actual class 1 instances are correctly identified).\n",
    "F1-score: The harmonic mean of precision and recall.\n",
    "\n",
    "Class 0: 0.67.\n",
    "Class 1: 0.57.\n",
    "Accuracy: Overall, 62% of the test set instances are correctly classified.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Class 0 (Control Group or Non-Amblyopia):\n",
    "\n",
    "High Recall (1.00): The model correctly identified all instances of class 0.\n",
    "Low Precision (0.50): Half of the instances predicted as class 0 are actually from class 1 (false positives).\n",
    "Class 1 (Amblyopia Group):\n",
    "\n",
    "High Precision (1.00): All instances predicted as class 1 are correctly from class 1.\n",
    "Low Recall (0.40): The model failed to identify 60% of actual class 1 instances (false negatives).\n",
    "Overall Performance:\n",
    "\n",
    "The model is better at identifying class 0 but struggles to correctly identify all instances of class 1.\n",
    "Accuracy is 62%, which may not be satisfactory depending on the context.\n",
    "Macro Average:\n",
    "Precision: 0.75.\n",
    "Recall: 0.70.\n",
    "F1-score: 0.62.\n",
    "Possible Reasons for the Performance:\n",
    "\n",
    "Class Imbalance: Although the classes are relatively balanced (6 vs. 10), the model may still be biased towards class 0.\n",
    "Small Dataset: With only 16 instances in the test set, the model's performance metrics may not be stable or representative.\n",
    "Overfitting: The model may have overfitted to the training data, especially if the training set is small or if the model is too complex.\n",
    "Feature Selection: Limiting to 10 features may have excluded important predictors.\n",
    "3. Important Features and Their Importances\n",
    "plaintext\n",
    "Copy code\n",
    "                      Feature  Importance\n",
    "4             O1__maximum    0.207673\n",
    "8    O2__absolute_maximum    0.140004\n",
    "7             O2__maximum    0.126442\n",
    "9             O2__minimum    0.125662\n",
    "1  O1__standard_deviation    0.092274\n",
    "5    O1__absolute_maximum    0.088193\n",
    "6             O1__minimum    0.083645\n",
    "3    O1__root_mean_square    0.077596\n",
    "0              O1__median    0.058510\n",
    "2            O1__variance    0.000000\n",
    "Feature Descriptions:\n",
    "\n",
    "Channel O1:\n",
    "\n",
    "O1__maximum: The maximum value of the EEG signal in the O1 channel.\n",
    "O1__absolute_maximum: The largest absolute value in the O1 channel.\n",
    "O1__minimum: The minimum value in the O1 channel.\n",
    "O1__standard_deviation: The standard deviation of the O1 signal.\n",
    "O1__root_mean_square: The RMS value of the O1 signal.\n",
    "O1__median: The median value of the O1 signal.\n",
    "O1__variance: The variance of the O1 signal.\n",
    "Channel O2:\n",
    "\n",
    "O2__absolute_maximum: The largest absolute value in the O2 channel.\n",
    "O2__maximum: The maximum value of the EEG signal in the O2 channel.\n",
    "O2__minimum: The minimum value in the O2 channel.\n",
    "Feature Importances:\n",
    "\n",
    "The importance values indicate the relative contribution of each feature to the model's decision-making.\n",
    "\n",
    "Top Features:\n",
    "\n",
    "O1__maximum (0.2077): Most significant feature.\n",
    "O2__absolute_maximum (0.1400).\n",
    "O2__maximum (0.1264).\n",
    "O2__minimum (0.1257).\n",
    "Zero Importance Feature:\n",
    "\n",
    "O1__variance (0.0000): This feature did not contribute to the model's predictions.\n",
    "Interpretation:\n",
    "\n",
    "The model heavily relies on maximum and minimum amplitude values from channels O1 and O2.\n",
    "Features like standard deviation and root mean square also play a role but are less significant.\n",
    "The variance of the O1 signal didn't contribute, possibly due to redundancy with other features or lack of discriminative power.\n",
    "4. Overall Analysis and Recommendations\n",
    "Understanding the Model's Behavior:\n",
    "\n",
    "High Recall for Class 0: The model correctly identifies all control group instances but at the cost of misclassifying many amblyopia instances as controls.\n",
    "Low Recall for Class 1: The model misses 60% of the amblyopia cases, which is critical if the goal is to detect amblyopia.\n",
    "Feature Dependence: The model's reliance on extreme values (maximum and minimum) may make it sensitive to noise or outliers in the data.\n",
    "Possible Issues:\n",
    "\n",
    "Data Quality: EEG data can be noisy. Extreme values may be influenced by artifacts rather than true neural activity.\n",
    "Overfitting to Noise: Focusing on maximum and minimum values might cause the model to capture noise rather than meaningful patterns.\n",
    "Small Sample Size: With a limited number of samples, especially in the test set, performance metrics may not be reliable.\n",
    "Feature Selection Limitations: Selecting only 10 features may not capture the complexity needed to differentiate between classes.\n",
    "Recommendations:\n",
    "\n",
    "Increase Dataset Size:\n",
    "\n",
    "Collect more EEG recordings to provide the model with more examples to learn from.\n",
    "Enhance Feature Extraction:\n",
    "\n",
    "Use more comprehensive feature extraction methods, possibly including frequency-domain features (e.g., power spectral density).\n",
    "Consider time-frequency analysis (e.g., wavelet transforms) to capture transient events.\n",
    "Feature Selection Strategy:\n",
    "\n",
    "Instead of selecting a fixed number of features (k), consider using all features or use techniques like recursive feature elimination (RFE) to find the optimal feature subset.\n",
    "Evaluate feature importance using different criteria, such as mutual information.\n",
    "Address Class Imbalance:\n",
    "\n",
    "Although the class distribution isn't severely imbalanced, using techniques like SMOTE (Synthetic Minority Over-sampling Technique) can help the model learn better representations of the minority class.\n",
    "Ensure that the train-test split maintains class distribution (stratified sampling).\n",
    "Model Tuning and Validation:\n",
    "\n",
    "Experiment with different models, such as Gradient Boosting Machines, Support Vector Machines, or Neural Networks.\n",
    "Use cross-validation to obtain a more reliable estimate of the model's performance.\n",
    "Adjust Evaluation Metrics:\n",
    "\n",
    "Since missing amblyopia cases is more critical, consider optimizing for recall on class 1.\n",
    "Use metrics like ROC AUC or Precision-Recall curves to get a better understanding of model performance.\n",
    "Data Preprocessing:\n",
    "\n",
    "Apply signal processing techniques to reduce noise, such as filtering or artifact rejection.\n",
    "Normalize or standardize the features to reduce the impact of scale differences.\n",
    "Investigate Feature Importances:\n",
    "\n",
    "Analyze why certain features are more important.\n",
    "Consider if these features make sense from a neuroscientific perspective.\n",
    "Conclusion\n",
    "Model Limitations: The current model doesn't perform adequately, especially in detecting the amblyopia class, which is critical for your application.\n",
    "Next Steps: Implement the recommendations to improve data quality, feature richness, and model robustness.\n",
    "Continuous Evaluation: As you make changes, continue to evaluate the model using appropriate metrics and validation strategies.\n",
    "Remember: Machine learning in healthcare and biomedical applications often requires careful consideration of data quality, feature engineering, and ethical implications of false negatives and false positives. It's crucial to ensure that the model is reliable and performs well on the aspects that matter most for the intended use case.\n",
    "\n",
    "Let me know if you have any questions or need further clarification on any of these points!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 15/15 [00:01<00:00,  8.57it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:01<00:00,  8.21it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:01<00:00,  8.45it/s]\n",
      "Feature Extraction: 100%|██████████| 14/14 [00:01<00:00,  8.12it/s]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [ 3 13 23] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 500}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.67      0.67      0.67        12\n",
      "weighted avg       0.67      0.67      0.67        12\n",
      "\n",
      "                  Feature  Importance\n",
      "8    O2__absolute_maximum    0.231114\n",
      "0          Oz__sum_values    0.165242\n",
      "7             O2__maximum    0.147457\n",
      "2          O2__sum_values    0.136516\n",
      "9             O2__minimum    0.133236\n",
      "4  O2__standard_deviation    0.104246\n",
      "6    O2__root_mean_square    0.082190\n",
      "1                Oz__mean    0.000000\n",
      "3                O2__mean    0.000000\n",
      "5            O2__variance    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# Define the function to process data in chunks using ComprehensiveFCParameters\n",
    "def process_in_chunks(time_series_df, N):\n",
    "    # Get unique IDs\n",
    "    unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the IDs in the current chunk\n",
    "        chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk using ComprehensiveFCParameters\n",
    "        extracted_features_chunk = extract_features(\n",
    "            chunk_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=MinimalFCParameters(),  # Use ComprehensiveFCParameters() for more features\n",
    "            n_jobs=4,  # Adjust based on your CPU cores\n",
    "            # Since data is in wide format, we do not need to specify column_kind and column_value\n",
    "        )\n",
    "        \n",
    "        # Impute missing values in the extracted features\n",
    "        impute(extracted_features_chunk)\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features_chunk)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_df, extracted_features_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Set the chunk size N (adjust based on your memory constraints)\n",
    "N = 10  # Smaller chunk size to manage memory usage\n",
    "\n",
    "# Extract features using the process_in_chunks function\n",
    "extracted_features = process_in_chunks(time_series_df, N)\n",
    "\n",
    "# Drop any columns with NaN or infinite values\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Ensure that the labels are aligned with the extracted features\n",
    "# Assuming 'labels' is a Series with 'id' as the index\n",
    "labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_features_clean,\n",
    "    labels_aligned,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=labels_aligned  # Ensure stratified sampling\n",
    ")\n",
    "\n",
    "# Select the most important features using ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=10)  # Adjust 'k' as needed\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best estimator from GridSearchCV to predict and evaluate the model\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Identify and display the top selected features with importance\n",
    "important_features = pd.DataFrame({\n",
    "    'Feature': selected_feature_names,\n",
    "    'Importance': best_clf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/e2/7b/8c1b410cd0604cee9a167a19f7e1746f5b92ae7d02ad574ab560b73c5a48/xgboost-2.1.1-py3-none-win_amd64.whl.metadata\n",
      "  Downloading xgboost-2.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\werpelga\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\werpelga\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.1-py3-none-win_amd64.whl (124.9 MB)\n",
      "   --------------------------------------- 124.9/124.9 MB 10.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 15/15 [00:02<00:00,  6.34it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:03<00:00,  4.25it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:02<00:00,  6.85it/s]\n",
      "Feature Extraction: 100%|██████████| 14/14 [00:02<00:00,  6.69it/s]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [ 3 13 23] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating Random Forest...\n",
      "Best parameters for Random Forest: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 500}\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.67      0.67      0.67        12\n",
      "weighted avg       0.67      0.67      0.67        12\n",
      "\n",
      "Important features for Random Forest:\n",
      "                  Feature  Importance\n",
      "8    O2__absolute_maximum    0.231114\n",
      "0          Oz__sum_values    0.165242\n",
      "7             O2__maximum    0.147457\n",
      "2          O2__sum_values    0.136516\n",
      "9             O2__minimum    0.133236\n",
      "4  O2__standard_deviation    0.104246\n",
      "6    O2__root_mean_square    0.082190\n",
      "1                Oz__mean    0.000000\n",
      "3                O2__mean    0.000000\n",
      "5            O2__variance    0.000000\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Best parameters for Logistic Regression: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.50      1.00      0.67         6\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.25      0.50      0.33        12\n",
      "weighted avg       0.25      0.50      0.33        12\n",
      "\n",
      "Important features for Logistic Regression:\n",
      "                  Feature    Importance\n",
      "0          Oz__sum_values  3.820798e-05\n",
      "2          O2__sum_values  3.620351e-05\n",
      "8    O2__absolute_maximum  1.023054e-06\n",
      "9             O2__minimum  9.369002e-07\n",
      "7             O2__maximum  7.694399e-07\n",
      "6    O2__root_mean_square  1.373536e-07\n",
      "4  O2__standard_deviation  1.373516e-07\n",
      "1                Oz__mean  2.332030e-09\n",
      "3                O2__mean  2.209687e-09\n",
      "5            O2__variance  1.284294e-12\n",
      "\n",
      "Training and evaluating Support Vector Machine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Support Vector Machine: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Classification Report for Support Vector Machine:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.33      0.50      0.40         6\n",
      "\n",
      "    accuracy                           0.25        12\n",
      "   macro avg       0.17      0.25      0.20        12\n",
      "weighted avg       0.17      0.25      0.20        12\n",
      "\n",
      "Support Vector Machine does not provide feature importances directly.\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.67      0.67      0.67        12\n",
      "weighted avg       0.67      0.67      0.67        12\n",
      "\n",
      "Important features for Gradient Boosting:\n",
      "                  Feature  Importance\n",
      "8    O2__absolute_maximum    0.315341\n",
      "0          Oz__sum_values    0.212833\n",
      "2          O2__sum_values    0.146715\n",
      "7             O2__maximum    0.115096\n",
      "6    O2__root_mean_square    0.094601\n",
      "4  O2__standard_deviation    0.090840\n",
      "9             O2__minimum    0.024573\n",
      "1                Oz__mean    0.000000\n",
      "3                O2__mean    0.000000\n",
      "5            O2__variance    0.000000\n",
      "\n",
      "Training and evaluating Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Neural Network: {'classifier__activation': 'relu', 'classifier__alpha': 0.0001, 'classifier__hidden_layer_sizes': (50,), 'classifier__learning_rate': 'constant', 'classifier__solver': 'sgd'}\n",
      "Classification Report for Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55         6\n",
      "           1       0.57      0.67      0.62         6\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.59      0.58      0.58        12\n",
      "weighted avg       0.59      0.58      0.58        12\n",
      "\n",
      "Neural Network does not provide feature importances directly.\n",
      "\n",
      "Training and evaluating XGBoost...\n",
      "Best parameters for XGBoost: {'colsample_bytree': 0.7, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.7}\n",
      "Classification Report for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       0.80      0.67      0.73         6\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.76      0.75      0.75        12\n",
      "weighted avg       0.76      0.75      0.75        12\n",
      "\n",
      "Important features for XGBoost:\n",
      "                  Feature  Importance\n",
      "8    O2__absolute_maximum    0.146500\n",
      "7             O2__maximum    0.122445\n",
      "3                O2__mean    0.120087\n",
      "5            O2__variance    0.104427\n",
      "6    O2__root_mean_square    0.095877\n",
      "2          O2__sum_values    0.087870\n",
      "4  O2__standard_deviation    0.087093\n",
      "0          Oz__sum_values    0.084408\n",
      "1                Oz__mean    0.079364\n",
      "9             O2__minimum    0.071930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:56:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier  # Import XGBoost classifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# Define the function to process data in chunks using ComprehensiveFCParameters\n",
    "def process_in_chunks(time_series_df, N):\n",
    "    # Get unique IDs\n",
    "    unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the IDs in the current chunk\n",
    "        chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk using ComprehensiveFCParameters\n",
    "        extracted_features_chunk = extract_features(\n",
    "            chunk_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=MinimalFCParameters(),  # Use ComprehensiveFCParameters() for more features\n",
    "            n_jobs=4,  # Adjust based on your CPU cores\n",
    "            # Since data is in wide format, we do not need to specify column_kind and column_value\n",
    "        )\n",
    "        \n",
    "        # Impute missing values in the extracted features\n",
    "        impute(extracted_features_chunk)\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features_chunk)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_df, extracted_features_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Set the chunk size N (adjust based on your memory constraints)\n",
    "N = 10  # Smaller chunk size to manage memory usage\n",
    "\n",
    "# Extract features using the process_in_chunks function\n",
    "extracted_features = process_in_chunks(time_series_df, N)\n",
    "\n",
    "# Drop any columns with NaN or infinite values\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Ensure that the labels are aligned with the extracted features\n",
    "# Assuming 'labels' is a Series with 'id' as the index\n",
    "labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_features_clean,\n",
    "    labels_aligned,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=labels_aligned  # Ensure stratified sampling\n",
    ")\n",
    "\n",
    "# Select the most important features using ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=10)  # Adjust 'k' as needed\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "\n",
    "# Initialize an empty dictionary to store classifiers and their parameter grids\n",
    "classifiers = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=5000),\n",
    "        'param_grid': {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['liblinear'],\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'param_grid': [\n",
    "            {'kernel': ['linear'], 'C': [0.1, 1, 10, 100]},\n",
    "            {'kernel': ['rbf'], 'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto']},\n",
    "            {'kernel': ['poly'], 'C': [0.1, 1, 10], 'degree': [2, 3], 'gamma': ['scale', 'auto']}\n",
    "        ]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5],\n",
    "            'min_samples_split': [2, 5],\n",
    "        }\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'model': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', MLPClassifier(random_state=42, max_iter=500))\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'classifier__activation': ['tanh', 'relu'],\n",
    "            'classifier__solver': ['adam', 'sgd'],\n",
    "            'classifier__alpha': [0.0001, 0.001],\n",
    "            'classifier__learning_rate': ['constant', 'adaptive'],\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.7, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop through each classifier, perform grid search, and evaluate\n",
    "for name, classifier_info in classifiers.items():\n",
    "    print(f\"\\nTraining and evaluating {name}...\")\n",
    "    model = classifier_info['model']\n",
    "    param_grid = classifier_info['param_grid']\n",
    "    \n",
    "    # For classifiers that include the feature selection or scaling in a pipeline, use X_train and X_test directly\n",
    "    if name == 'Neural Network':\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_clf = grid_search.best_estimator_\n",
    "        y_pred = best_clf.predict(X_test)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        best_clf = grid_search.best_estimator_\n",
    "        y_pred = best_clf.predict(X_test_selected)\n",
    "    \n",
    "    # Print the best parameters found by GridSearchCV\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # For models that provide feature importances, display them\n",
    "    if hasattr(best_clf, 'feature_importances_'):\n",
    "        important_features = pd.DataFrame({\n",
    "            'Feature': selected_feature_names,\n",
    "            'Importance': best_clf.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(f\"Important features for {name}:\")\n",
    "        print(important_features)\n",
    "    elif hasattr(best_clf, 'coef_'):\n",
    "        # For linear models like Logistic Regression\n",
    "        importance = np.abs(best_clf.coef_[0])\n",
    "        important_features = pd.DataFrame({\n",
    "            'Feature': selected_feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(f\"Important features for {name}:\")\n",
    "        print(important_features)\n",
    "    else:\n",
    "        print(f\"{name} does not provide feature importances directly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model saved to 'best_xgboost_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'best_clf' is your best XGBoost classifier from GridSearchCV\n",
    "# if name == 'XGBoost':\n",
    "#     # Save the best XGBoost model using joblib\n",
    "#     joblib.dump(best_clf, 'best_xgboost_model.pkl')\n",
    "#     print(\"XGBoost model saved to 'best_xgboost_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# # Save the trained classifier\n",
    "# joblib.dump(best_clf, 'trained_random_forest.pkl')\n",
    "\n",
    "# # Save the feature selector\n",
    "# joblib.dump(selector, 'feature_selector.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
