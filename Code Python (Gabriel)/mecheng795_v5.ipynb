{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mne scipy\n",
    "#!pip install pandas numpy openpyxl\n",
    "#!pip install tsfresh\n",
    "#!pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import mne\n",
    "\n",
    "def process_all_eeg_data() -> dict:\n",
    "    \"\"\"\n",
    "    Process all .bdf EEG files in the current directory, applying filters and extracting data from\n",
    "    channels A15 (O1), A16 (Oz), and A17 (O2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing processed EEG data and header information for each file.\n",
    "    \"\"\"\n",
    "    # Get a list of all .bdf files in the current directory\n",
    "    files = [f for f in os.listdir('.') if f.endswith('.bdf')]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No BDF files found in the current directory\")\n",
    "    \n",
    "    # Initialize the results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Loop over each file\n",
    "    for filename in files:\n",
    "        full_file_path = os.path.join(os.getcwd(), filename)\n",
    "        \n",
    "        # Read the raw EEG data using MNE\n",
    "        raw = mne.io.read_raw_bdf(full_file_path, preload=True)\n",
    "        hdr = raw.info\n",
    "        \n",
    "        # Select data from channels A15 (O1), A16 (Oz), and A17 (O2)\n",
    "        channels_select = ['A15', 'A16', 'A17']\n",
    "        missing_channels = [ch for ch in channels_select if ch not in hdr['ch_names']]\n",
    "        if missing_channels:\n",
    "            raise ValueError(f\"Selected channels {missing_channels} not found in the data\")\n",
    "        \n",
    "        channel_indices = [hdr['ch_names'].index(ch) for ch in channels_select]\n",
    "        EEG_data = raw.get_data(picks=channel_indices).T  # Shape: (n_samples, n_channels)\n",
    "        \n",
    "        # Filter EEG Data\n",
    "        Fs = hdr['sfreq']  # Sampling frequency\n",
    "        \n",
    "        # Bandpass filter parameters (2 to 80 Hz)\n",
    "        Fc_BP = [2, 80]  # Bandpass frequency range\n",
    "        Wn_BP = [f / (Fs / 2) for f in Fc_BP]  # Normalize by Nyquist frequency\n",
    "        \n",
    "        # Create and apply bandpass filter (6th order zero-phase Butterworth IIR)\n",
    "        B_BP, A_BP = signal.butter(3, Wn_BP, btype='bandpass')\n",
    "        EEG_filtered_BP = signal.filtfilt(B_BP, A_BP, EEG_data, axis=0)\n",
    "        \n",
    "        # Band stop filter parameters (48 to 52 Hz)\n",
    "        Fc_BS = [48, 52]  # Band stop frequency range\n",
    "        Wn_BS = [f / (Fs / 2) for f in Fc_BS]  # Normalize by Nyquist frequency\n",
    "        \n",
    "        # Create and apply band stop filter (6th order zero-phase Butterworth IIR)\n",
    "        B_BS, A_BS = signal.butter(3, Wn_BS, btype='bandstop')\n",
    "        EEG_filtered = signal.filtfilt(B_BS, A_BS, EEG_filtered_BP, axis=0)\n",
    "        \n",
    "        # Extract prefix before underscore from the filename\n",
    "        underscore_index = filename.find('_')\n",
    "        if underscore_index == -1:\n",
    "            raise ValueError(f\"Filename format error, no underscore found in {filename}\")\n",
    "        key = filename[:underscore_index]\n",
    "        \n",
    "        # Store results in the dictionary\n",
    "        results[key] = {\n",
    "            'data': EEG_filtered,      # Filtered data for channels A15, A16, A17\n",
    "            'channels': channels_select,  # List of channel names\n",
    "            'header': hdr\n",
    "        }\n",
    "        \n",
    "        # Display a message indicating successful processing\n",
    "        print(f\"Data for file {filename} processed successfully\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A1_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 739327  =      0.000 ...   361.000 secs...\n",
      "Data for file A1_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A3_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 757759  =      0.000 ...   370.000 secs...\n",
      "Data for file A3_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A4_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 782335  =      0.000 ...   382.000 secs...\n",
      "Data for file A4_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A6_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A6_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A7_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A7_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A8_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A8_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A9_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A9_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C11_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C11_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C12_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C12_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C13_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C13_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C14_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C14_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C15_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C15_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C1_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C1_Alpha.bdf processed successfully\n"
     ]
    }
   ],
   "source": [
    "results = process_all_eeg_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def segment_eeg_data_new(results: dict, cohort_file: str = 'Cohort.xlsx') -> dict:\n",
    "    \"\"\"\n",
    "    Segments EEG data into predefined sections (EC, EO, LC, RC, DEC, NDEC) based on cohort information,\n",
    "    removing the first 2 seconds from each section.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        Dictionary containing the raw EEG data and header information for each key (participant).\n",
    "    cohort_file : str, optional\n",
    "        Path to the Excel file containing cohort information (default is 'Cohort.xlsx').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing segmented EEG data for each participant.\n",
    "    \"\"\"\n",
    "    # Read the cohort information from an Excel file\n",
    "    cohort_table = pd.read_excel(cohort_file)\n",
    "    # Segment Duration (in seconds)\n",
    "    segment_duration = 10  # Original segment duration in seconds\n",
    "    skip_duration = 2      # Duration to skip at the start of each segment (2 seconds)\n",
    "\n",
    "    # Initialize the segmented results dictionary\n",
    "    segmented_data = {}\n",
    "\n",
    "    # Iterate through each key in the results dictionary\n",
    "    for key, result in results.items():\n",
    "        data = result['data']  # Data shape: (n_samples, n_channels)\n",
    "        hdr = result['header']\n",
    "\n",
    "        # Find the matching row in the cohort table\n",
    "        cohort_row = cohort_table[cohort_table['Cohort'] == key]\n",
    "        \n",
    "        if cohort_row.empty:\n",
    "            raise ValueError(f\"Cohort information not found for {key}\")\n",
    "\n",
    "        # Define the sample rate and calculate sample counts\n",
    "        Fs = hdr['sfreq']  # Sampling frequency\n",
    "        samples_per_segment = int(segment_duration * Fs)\n",
    "        samples_to_skip = int(skip_duration * Fs)\n",
    "        effective_samples_per_segment = samples_per_segment - samples_to_skip\n",
    "        n_channels = data.shape[1]  # Number of channels (should be 3: O1, Oz, O2)\n",
    "\n",
    "        # Initialize segments with zeros\n",
    "        EC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        EO = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        LC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        RC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        DEC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        NDEC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "\n",
    "        # Fill segments with data if available, skipping the first 2 seconds\n",
    "        # EC segment\n",
    "        segment_start = 0\n",
    "        segment_end = samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            EC = data[segment_start + samples_to_skip : 0, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for EC segment in {key}\")\n",
    "\n",
    "        # EO segment\n",
    "        segment_start = samples_per_segment\n",
    "        segment_end = 2 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            EO = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for EO segment in {key}\")\n",
    "\n",
    "        # LC segment\n",
    "        segment_start = 2 * samples_per_segment\n",
    "        segment_end = 3 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            LC = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for LC segment in {key}\")\n",
    "\n",
    "        # RC segment\n",
    "        segment_start = 3 * samples_per_segment\n",
    "        segment_end = 4 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            RC = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for RC segment in {key}\")\n",
    "\n",
    "        # Apply conditions based on cohort table\n",
    "        if cohort_row['LC'].values[0] == 'DEC':\n",
    "            # Assign 'DEC' to LC and 'NDEC' to RC\n",
    "            DEC = LC\n",
    "            NDEC = RC\n",
    "        elif cohort_row['RC'].values[0] == 'DEC':\n",
    "            # Assign 'DEC' to RC and 'NDEC' to LC\n",
    "            DEC = RC\n",
    "            NDEC = LC\n",
    "        else:\n",
    "            # If neither LC nor RC is 'DEC', assign NDEC accordingly\n",
    "            NDEC = LC\n",
    "            # Optionally handle cases where DEC is not specified\n",
    "            DEC = RC  # Or set DEC to zeros if appropriate\n",
    "\n",
    "        # Store the segmented data and 'LinesDifference' in the results dictionary\n",
    "        segmented_data[key] = {\n",
    "            'header': hdr,\n",
    "            'EC': EC,\n",
    "            'EO': EO,\n",
    "            'DEC': DEC,\n",
    "            'NDEC': NDEC,\n",
    "            'LinesDifference': cohort_row['LinesDifference'].values[0]\n",
    "        }\n",
    "\n",
    "    return segmented_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data = segment_eeg_data_new(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def prepare_time_series_by_section(segmented_data, cohort_table):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame suitable for tsfresh from segmented EEG data for all sections (EC, EO, DEC, NDEC).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    segmented_data : dict\n",
    "        The dictionary containing segmented EEG data for each participant.\n",
    "    cohort_table : pd.DataFrame\n",
    "        DataFrame containing cohort information (including labels for Amblyopia/Control).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame, pd.Series\n",
    "        A DataFrame where each row represents a time-series sample with columns 'id', 'time', 'O1', 'Oz', 'O2',\n",
    "        and a Series with group labels indexed by 'id'.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Loop through each participant's data\n",
    "    for key, value in segmented_data.items():\n",
    "        # Find the matching cohort row\n",
    "        cohort_row = cohort_table[cohort_table['Cohort'] == key]\n",
    "        if cohort_row.empty:\n",
    "            continue\n",
    "\n",
    "        # Assign label based on the first letter of the 'Cohort' column (Amblyopia = 1, Control = 0)\n",
    "        label = 1 if key.startswith('A') else 0\n",
    "\n",
    "        # Get channel names; default to ['O1', 'Oz', 'O2'] if not available\n",
    "        channels = value.get('channels', ['O1', 'Oz', 'O2'])\n",
    "\n",
    "        # For each section (EC, EO, DEC, NDEC)\n",
    "        for section in ['EC', 'EO', 'DEC', 'NDEC']:\n",
    "            section_data = value[section]  # Shape: (n_samples, n_channels)\n",
    "\n",
    "            # Create a DataFrame for this section\n",
    "            n_samples = section_data.shape[0]\n",
    "            df = pd.DataFrame({\n",
    "                'id': f\"{key}_{section}\",\n",
    "                'time': np.arange(n_samples)\n",
    "            })\n",
    "\n",
    "            # Add each channel's data as a column\n",
    "            for idx, channel_name in enumerate(channels):\n",
    "                df[channel_name] = section_data[:, idx]\n",
    "\n",
    "            # Append to data list\n",
    "            data_list.append(df)\n",
    "\n",
    "            # Append label for this 'id' (participant_section)\n",
    "            labels_list.append({'id': f\"{key}_{section}\", 'label': label})\n",
    "\n",
    "    # Concatenate all data into a single DataFrame\n",
    "    time_series_df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "    # Create a labels DataFrame and convert to a Series indexed by 'id'\n",
    "    labels_df = pd.DataFrame(labels_list).drop_duplicates(subset='id')\n",
    "    labels_series = labels_df.set_index('id')['label']\n",
    "\n",
    "    # Return the time-series data and corresponding labels\n",
    "    return time_series_df, labels_series\n",
    "\n",
    "# Load your cohort table (must include 'Cohort' column)\n",
    "cohort_table = pd.read_excel('Cohort.xlsx')\n",
    "\n",
    "# Prepare the time series DataFrame and labels\n",
    "time_series_df, labels = prepare_time_series_by_section(segmented_data, cohort_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>O1</th>\n",
       "      <th>Oz</th>\n",
       "      <th>O2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1_EO</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1_EO</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1_EO</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1_EO</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1_EO</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638971</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>16379</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638972</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>16380</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638973</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>16381</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638974</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>16382</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638975</th>\n",
       "      <td>C1_NDEC</td>\n",
       "      <td>16383</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>638976 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id   time        O1        Oz        O2\n",
       "0         A1_EO      0 -0.000013 -0.000026 -0.000036\n",
       "1         A1_EO      1 -0.000016 -0.000028 -0.000038\n",
       "2         A1_EO      2 -0.000019 -0.000031 -0.000040\n",
       "3         A1_EO      3 -0.000022 -0.000034 -0.000042\n",
       "4         A1_EO      4 -0.000025 -0.000036 -0.000043\n",
       "...         ...    ...       ...       ...       ...\n",
       "638971  C1_NDEC  16379 -0.000003 -0.000004 -0.000004\n",
       "638972  C1_NDEC  16380 -0.000003 -0.000003 -0.000004\n",
       "638973  C1_NDEC  16381 -0.000003 -0.000002 -0.000003\n",
       "638974  C1_NDEC  16382 -0.000003 -0.000002 -0.000003\n",
       "638975  C1_NDEC  16383 -0.000003 -0.000002 -0.000002\n",
       "\n",
       "[638976 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save time_series_df as CSV\n",
    "time_series_df.to_csv('time_series_df_full.csv', index=False)\n",
    "\n",
    "# Save labels as CSV\n",
    "labels.to_csv('labels_full.csv', index=False, header=True)\n",
    "\n",
    "# Optionally, save labels as Pickle (preserves Python object types)\n",
    "# labels.to_pickle('labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read time_series_df from CSV\n",
    "# time_series_df = pd.read_csv('time_series_df_full.csv')\n",
    "\n",
    "# # Read labels from CSV\n",
    "# labels = pd.read_csv('labels_full.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.index = time_series_df['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 15/15 [00:03<00:00,  4.58it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:03<00:00,  4.64it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:03<00:00,  4.49it/s]\n",
      "Feature Extraction: 100%|██████████| 14/14 [00:03<00:00,  4.30it/s]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [ 3 13 23] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating Random Forest...\n",
      "Best parameters for Random Forest: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 500}\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57         4\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.62         8\n",
      "   macro avg       0.63      0.62      0.62         8\n",
      "weighted avg       0.63      0.62      0.62         8\n",
      "\n",
      "Important features for Random Forest:\n",
      "                  Feature  Importance\n",
      "8    O2__absolute_maximum    0.244365\n",
      "0          Oz__sum_values    0.134510\n",
      "7             O2__maximum    0.131653\n",
      "4  O2__standard_deviation    0.130236\n",
      "9             O2__minimum    0.129713\n",
      "6    O2__root_mean_square    0.126302\n",
      "2          O2__sum_values    0.103221\n",
      "1                Oz__mean    0.000000\n",
      "3                O2__mean    0.000000\n",
      "5            O2__variance    0.000000\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Best parameters for Logistic Regression: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Classification Report for Logistic Regression:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.50      1.00      0.67         4\n",
      "\n",
      "    accuracy                           0.50         8\n",
      "   macro avg       0.25      0.50      0.33         8\n",
      "weighted avg       0.25      0.50      0.33         8\n",
      "\n",
      "Important features for Logistic Regression:\n",
      "                  Feature    Importance\n",
      "0          Oz__sum_values  3.034310e-05\n",
      "2          O2__sum_values  2.794456e-05\n",
      "8    O2__absolute_maximum  1.114411e-06\n",
      "9             O2__minimum  1.031037e-06\n",
      "7             O2__maximum  8.252159e-07\n",
      "6    O2__root_mean_square  1.613520e-07\n",
      "4  O2__standard_deviation  1.613487e-07\n",
      "1                Oz__mean  1.851996e-09\n",
      "3                O2__mean  1.705601e-09\n",
      "5            O2__variance  1.585562e-12\n",
      "\n",
      "Training and evaluating Support Vector Machine...\n",
      "Best parameters for Support Vector Machine: {'C': 0.1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Classification Report for Support Vector Machine:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.50      1.00      0.67         4\n",
      "\n",
      "    accuracy                           0.50         8\n",
      "   macro avg       0.25      0.50      0.33         8\n",
      "weighted avg       0.25      0.50      0.33         8\n",
      "\n",
      "Support Vector Machine does not provide feature importances directly.\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57         4\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.62         8\n",
      "   macro avg       0.63      0.62      0.62         8\n",
      "weighted avg       0.63      0.62      0.62         8\n",
      "\n",
      "Important features for Gradient Boosting:\n",
      "                  Feature  Importance\n",
      "8    O2__absolute_maximum    0.409995\n",
      "0          Oz__sum_values    0.116827\n",
      "7             O2__maximum    0.109700\n",
      "6    O2__root_mean_square    0.102658\n",
      "2          O2__sum_values    0.089874\n",
      "9             O2__minimum    0.088785\n",
      "4  O2__standard_deviation    0.082161\n",
      "1                Oz__mean    0.000000\n",
      "3                O2__mean    0.000000\n",
      "5            O2__variance    0.000000\n",
      "\n",
      "Training and evaluating Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Neural Network: {'classifier__activation': 'relu', 'classifier__alpha': 0.0001, 'classifier__hidden_layer_sizes': (50, 50), 'classifier__learning_rate': 'constant', 'classifier__solver': 'sgd'}\n",
      "Classification Report for Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         4\n",
      "           1       0.67      1.00      0.80         4\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.83      0.75      0.73         8\n",
      "weighted avg       0.83      0.75      0.73         8\n",
      "\n",
      "Neural Network does not provide feature importances directly.\n",
      "\n",
      "Training and evaluating XGBoost...\n",
      "Best parameters for XGBoost: {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "Classification Report for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75         4\n",
      "           1       0.75      0.75      0.75         4\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.75      0.75      0.75         8\n",
      "weighted avg       0.75      0.75      0.75         8\n",
      "\n",
      "Important features for XGBoost:\n",
      "                  Feature  Importance\n",
      "8    O2__absolute_maximum    0.184468\n",
      "9             O2__minimum    0.169886\n",
      "3                O2__mean    0.134066\n",
      "7             O2__maximum    0.106136\n",
      "4  O2__standard_deviation    0.089661\n",
      "5            O2__variance    0.082367\n",
      "0          Oz__sum_values    0.076186\n",
      "6    O2__root_mean_square    0.074073\n",
      "1                Oz__mean    0.055288\n",
      "2          O2__sum_values    0.027869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:37:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier  # Import XGBoost classifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# Define the function to process data in chunks using ComprehensiveFCParameters\n",
    "def process_in_chunks(time_series_df, N):\n",
    "    # Get unique IDs\n",
    "    unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the IDs in the current chunk\n",
    "        chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk using ComprehensiveFCParameters\n",
    "        extracted_features_chunk = extract_features(\n",
    "            chunk_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=MinimalFCParameters(),  # Use ComprehensiveFCParameters() for more features\n",
    "            n_jobs=4,  # Adjust based on your CPU cores\n",
    "            # Since data is in wide format, we do not need to specify column_kind and column_value\n",
    "        )\n",
    "        \n",
    "        # Impute missing values in the extracted features\n",
    "        impute(extracted_features_chunk)\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features_chunk)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_df, extracted_features_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Set the chunk size N (adjust based on your memory constraints)\n",
    "N = 10  # Smaller chunk size to manage memory usage\n",
    "\n",
    "# Extract features using the process_in_chunks function\n",
    "extracted_features = process_in_chunks(time_series_df, N)\n",
    "\n",
    "# Drop any columns with NaN or infinite values\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Ensure that the labels are aligned with the extracted features\n",
    "# Assuming 'labels' is a Series with 'id' as the index\n",
    "labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_features_clean,\n",
    "    labels_aligned,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels_aligned  # Ensure stratified sampling\n",
    ")\n",
    "\n",
    "# Select the most important features using ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=10)  # Adjust 'k' as needed\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "\n",
    "# Initialize an empty dictionary to store classifiers and their parameter grids\n",
    "classifiers = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=5000),\n",
    "        'param_grid': {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['liblinear'],\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'param_grid': [\n",
    "            {'kernel': ['linear'], 'C': [0.1, 1, 10, 100]},\n",
    "            {'kernel': ['rbf'], 'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto']},\n",
    "            {'kernel': ['poly'], 'C': [0.1, 1, 10], 'degree': [2, 3], 'gamma': ['scale', 'auto']}\n",
    "        ]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5],\n",
    "            'min_samples_split': [2, 5],\n",
    "        }\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'model': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', MLPClassifier(random_state=42, max_iter=500))\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'classifier__activation': ['tanh', 'relu'],\n",
    "            'classifier__solver': ['adam', 'sgd'],\n",
    "            'classifier__alpha': [0.0001, 0.001],\n",
    "            'classifier__learning_rate': ['constant', 'adaptive'],\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200, 300, 500],\n",
    "            'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.5, 0.9],\n",
    "            'subsample': [0.5, 0.7, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.5, 0.7, 0.8, 1.0],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop through each classifier, perform grid search, and evaluate\n",
    "for name, classifier_info in classifiers.items():\n",
    "    print(f\"\\nTraining and evaluating {name}...\")\n",
    "    model = classifier_info['model']\n",
    "    param_grid = classifier_info['param_grid']\n",
    "    \n",
    "    # For classifiers that include the feature selection or scaling in a pipeline, use X_train and X_test directly\n",
    "    if name == 'Neural Network':\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_clf = grid_search.best_estimator_\n",
    "        y_pred = best_clf.predict(X_test)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        best_clf = grid_search.best_estimator_\n",
    "        y_pred = best_clf.predict(X_test_selected)\n",
    "    \n",
    "    # Print the best parameters found by GridSearchCV\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # For models that provide feature importances, display them\n",
    "    if hasattr(best_clf, 'feature_importances_'):\n",
    "        important_features = pd.DataFrame({\n",
    "            'Feature': selected_feature_names,\n",
    "            'Importance': best_clf.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(f\"Important features for {name}:\")\n",
    "        print(important_features)\n",
    "    elif hasattr(best_clf, 'coef_'):\n",
    "        # For linear models like Logistic Regression\n",
    "        importance = np.abs(best_clf.coef_[0])\n",
    "        important_features = pd.DataFrame({\n",
    "            'Feature': selected_feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(f\"Important features for {name}:\")\n",
    "        print(important_features)\n",
    "    else:\n",
    "        print(f\"{name} does not provide feature importances directly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'best_clf' is your best XGBoost classifier from GridSearchCV\n",
    "# if name == 'XGBoost':\n",
    "#     # Save the best XGBoost model using joblib\n",
    "#     joblib.dump(best_clf, 'best_xgboost_model.pkl')\n",
    "#     print(\"XGBoost model saved to 'best_xgboost_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# # Save the trained classifier\n",
    "# joblib.dump(best_clf, 'trained_random_forest.pkl')\n",
    "\n",
    "# # Save the feature selector\n",
    "# joblib.dump(selector, 'feature_selector.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
