{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mne scipy\n",
    "#!pip install pandas numpy openpyxl\n",
    "#!pip install tsfresh\n",
    "#!pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import mne\n",
    "\n",
    "def process_all_eeg_data() -> dict:\n",
    "    \"\"\"\n",
    "    Process all .bdf EEG files in the current directory, applying filters and extracting data from\n",
    "    channels A15 (O1), A16 (Oz), and A17 (O2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing processed EEG data and header information for each file.\n",
    "    \"\"\"\n",
    "    # Get a list of all .bdf files in the current directory\n",
    "    files = [f for f in os.listdir('.') if f.endswith('.bdf')]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No BDF files found in the current directory\")\n",
    "    \n",
    "    # Initialize the results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Loop over each file\n",
    "    for filename in files:\n",
    "        full_file_path = os.path.join(os.getcwd(), filename)\n",
    "        \n",
    "        # Read the raw EEG data using MNE\n",
    "        raw = mne.io.read_raw_bdf(full_file_path, preload=True)\n",
    "        hdr = raw.info\n",
    "        \n",
    "        # Select data from channels A15 (O1), A16 (Oz), and A17 (O2)\n",
    "        channels_select = ['A15', 'A16', 'A17']\n",
    "        missing_channels = [ch for ch in channels_select if ch not in hdr['ch_names']]\n",
    "        if missing_channels:\n",
    "            raise ValueError(f\"Selected channels {missing_channels} not found in the data\")\n",
    "        \n",
    "        channel_indices = [hdr['ch_names'].index(ch) for ch in channels_select]\n",
    "        EEG_data = raw.get_data(picks=channel_indices).T  # Shape: (n_samples, n_channels)\n",
    "        \n",
    "        # Filter EEG Data\n",
    "        Fs = hdr['sfreq']  # Sampling frequency\n",
    "        \n",
    "        # Bandpass filter parameters (2 to 80 Hz)\n",
    "        Fc_BP = [2, 80]  # Bandpass frequency range\n",
    "        Wn_BP = [f / (Fs / 2) for f in Fc_BP]  # Normalize by Nyquist frequency\n",
    "        \n",
    "        # Create and apply bandpass filter (6th order zero-phase Butterworth IIR)\n",
    "        B_BP, A_BP = signal.butter(3, Wn_BP, btype='bandpass')\n",
    "        EEG_filtered_BP = signal.filtfilt(B_BP, A_BP, EEG_data, axis=0)\n",
    "        \n",
    "        # Band stop filter parameters (48 to 52 Hz)\n",
    "        Fc_BS = [48, 52]  # Band stop frequency range\n",
    "        Wn_BS = [f / (Fs / 2) for f in Fc_BS]  # Normalize by Nyquist frequency\n",
    "        \n",
    "        # Create and apply band stop filter (6th order zero-phase Butterworth IIR)\n",
    "        B_BS, A_BS = signal.butter(3, Wn_BS, btype='bandstop')\n",
    "        EEG_filtered = signal.filtfilt(B_BS, A_BS, EEG_filtered_BP, axis=0)\n",
    "        \n",
    "        # Extract prefix before underscore from the filename\n",
    "        underscore_index = filename.find('_')\n",
    "        if underscore_index == -1:\n",
    "            raise ValueError(f\"Filename format error, no underscore found in {filename}\")\n",
    "        key = filename[:underscore_index]\n",
    "        \n",
    "        # Store results in the dictionary\n",
    "        results[key] = {\n",
    "            'data': EEG_filtered,      # Filtered data for channels A15, A16, A17\n",
    "            'channels': channels_select,  # List of channel names\n",
    "            'header': hdr\n",
    "        }\n",
    "        \n",
    "        # Display a message indicating successful processing\n",
    "        print(f\"Data for file {filename} processed successfully\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A1_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 739327  =      0.000 ...   361.000 secs...\n",
      "Data for file A1_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A3_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 757759  =      0.000 ...   370.000 secs...\n",
      "Data for file A3_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A4_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 782335  =      0.000 ...   382.000 secs...\n",
      "Data for file A4_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A6_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A6_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A7_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A7_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A8_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A8_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A9_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A9_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C11_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C11_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C12_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C12_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C13_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C13_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C14_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C14_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C15_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C15_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C1_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C1_Alpha.bdf processed successfully\n"
     ]
    }
   ],
   "source": [
    "results = process_all_eeg_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def segment_eeg_data_new(results: dict, cohort_file: str = 'Cohort.xlsx') -> dict:\n",
    "    \"\"\"\n",
    "    Segments EEG data into predefined sections (EC, EO, LC, RC, DEC, NDEC) based on cohort information,\n",
    "    removing the first 2 seconds from each section.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        Dictionary containing the raw EEG data and header information for each key (participant).\n",
    "    cohort_file : str, optional\n",
    "        Path to the Excel file containing cohort information (default is 'Cohort.xlsx').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing segmented EEG data for each participant.\n",
    "    \"\"\"\n",
    "    # Read the cohort information from an Excel file\n",
    "    cohort_table = pd.read_excel(cohort_file)\n",
    "    # Segment Duration (in seconds)\n",
    "    segment_duration = 10  # Original segment duration in seconds\n",
    "    skip_duration = 2      # Duration to skip at the start of each segment (2 seconds)\n",
    "\n",
    "    # Initialize the segmented results dictionary\n",
    "    segmented_data = {}\n",
    "\n",
    "    # Iterate through each key in the results dictionary\n",
    "    for key, result in results.items():\n",
    "        data = result['data']  # Data shape: (n_samples, n_channels)\n",
    "        hdr = result['header']\n",
    "\n",
    "        # Find the matching row in the cohort table\n",
    "        cohort_row = cohort_table[cohort_table['Cohort'] == key]\n",
    "        \n",
    "        if cohort_row.empty:\n",
    "            raise ValueError(f\"Cohort information not found for {key}\")\n",
    "\n",
    "        # Define the sample rate and calculate sample counts\n",
    "        Fs = hdr['sfreq']  # Sampling frequency\n",
    "        samples_per_segment = int(segment_duration * Fs)\n",
    "        samples_to_skip = int(skip_duration * Fs)\n",
    "        effective_samples_per_segment = samples_per_segment - samples_to_skip\n",
    "        n_channels = data.shape[1]  # Number of channels (should be 3: O1, Oz, O2)\n",
    "\n",
    "        # Initialize segments with zeros\n",
    "        EC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        EO = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        LC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        RC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        DEC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        NDEC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "\n",
    "        # Fill segments with data if available, skipping the first 2 seconds\n",
    "        # EC segment\n",
    "        segment_start = 0\n",
    "        segment_end = samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            EC = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for EC segment in {key}\")\n",
    "\n",
    "        # EO segment\n",
    "        segment_start = samples_per_segment\n",
    "        segment_end = 2 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            EO = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for EO segment in {key}\")\n",
    "\n",
    "        # LC segment\n",
    "        segment_start = 2 * samples_per_segment\n",
    "        segment_end = 3 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            LC = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for LC segment in {key}\")\n",
    "\n",
    "        # RC segment\n",
    "        segment_start = 3 * samples_per_segment\n",
    "        segment_end = 4 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            RC = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for RC segment in {key}\")\n",
    "\n",
    "        # Apply conditions based on cohort table\n",
    "        if cohort_row['LC'].values[0] == 'DEC':\n",
    "            # Assign 'DEC' to LC and 'NDEC' to RC\n",
    "            DEC = LC\n",
    "            NDEC = RC\n",
    "        elif cohort_row['RC'].values[0] == 'DEC':\n",
    "            # Assign 'DEC' to RC and 'NDEC' to LC\n",
    "            DEC = RC\n",
    "            NDEC = LC\n",
    "        else:\n",
    "            # If neither LC nor RC is 'DEC', assign NDEC accordingly\n",
    "            NDEC = LC\n",
    "            # Optionally handle cases where DEC is not specified\n",
    "            DEC = RC  # Or set DEC to zeros if appropriate\n",
    "\n",
    "        # Store the segmented data and 'LinesDifference' in the results dictionary\n",
    "        segmented_data[key] = {\n",
    "            'header': hdr,\n",
    "            'EC': EC,\n",
    "            'EO': EO,\n",
    "            'DEC': DEC,\n",
    "            'NDEC': NDEC,\n",
    "            'LinesDifference': cohort_row['LinesDifference'].values[0]\n",
    "        }\n",
    "\n",
    "    return segmented_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data = segment_eeg_data_new(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def prepare_time_series_by_section(segmented_data, cohort_table):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame suitable for tsfresh from segmented EEG data for all sections (EC, EO, DEC, NDEC).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    segmented_data : dict\n",
    "        The dictionary containing segmented EEG data for each participant.\n",
    "    cohort_table : pd.DataFrame\n",
    "        DataFrame containing cohort information (including labels for Amblyopia/Control).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame, pd.Series\n",
    "        A DataFrame where each row represents a time-series sample with columns 'id', 'time', 'O1', 'Oz', 'O2',\n",
    "        and a Series with group labels indexed by 'id'.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Loop through each participant's data\n",
    "    for key, value in segmented_data.items():\n",
    "        # Find the matching cohort row\n",
    "        cohort_row = cohort_table[cohort_table['Cohort'] == key]\n",
    "        if cohort_row.empty:\n",
    "            continue\n",
    "\n",
    "        # Assign label based on the first letter of the 'Cohort' column (Amblyopia = 1, Control = 0)\n",
    "        label = 1 if key.startswith('A') else 0\n",
    "\n",
    "        # Get channel names; default to ['O1', 'Oz', 'O2'] if not available\n",
    "        channels = value.get('channels', ['O1', 'Oz', 'O2'])\n",
    "\n",
    "        # For each section (EC, EO, DEC, NDEC)\n",
    "        for section in ['EC', 'EO', 'DEC', 'NDEC']:\n",
    "            section_data = value[section]  # Shape: (n_samples, n_channels)\n",
    "\n",
    "            # Create a DataFrame for this section\n",
    "            n_samples = section_data.shape[0]\n",
    "            df = pd.DataFrame({\n",
    "                'id': f\"{key}_{section}\",\n",
    "                'time': np.arange(n_samples)\n",
    "            })\n",
    "\n",
    "            # Add each channel's data as a column\n",
    "            for idx, channel_name in enumerate(channels):\n",
    "                df[channel_name] = section_data[:, idx]\n",
    "\n",
    "            # Append to data list\n",
    "            data_list.append(df)\n",
    "\n",
    "            # Append label for this 'id' (participant_section)\n",
    "            labels_list.append({'id': f\"{key}_{section}\", 'label': label})\n",
    "\n",
    "    # Concatenate all data into a single DataFrame\n",
    "    time_series_df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "    # Create a labels DataFrame and convert to a Series indexed by 'id'\n",
    "    labels_df = pd.DataFrame(labels_list).drop_duplicates(subset='id')\n",
    "    labels_series = labels_df.set_index('id')['label']\n",
    "\n",
    "    # Return the time-series data and corresponding labels\n",
    "    return time_series_df, labels_series\n",
    "\n",
    "# Load your cohort table (must include 'Cohort' column)\n",
    "cohort_table = pd.read_excel('Cohort.xlsx')\n",
    "\n",
    "# Prepare the time series DataFrame and labels\n",
    "time_series_df, labels = prepare_time_series_by_section(segmented_data, cohort_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WERPELGA\\AppData\\Local\\Temp\\ipykernel_22744\\2895359124.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_without_one_amblyope = df_without_one_amblyope[time_series_df['id'] != amblyope_ids[1]].copy()\n",
      "C:\\Users\\WERPELGA\\AppData\\Local\\Temp\\ipykernel_22744\\2895359124.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_without_one_amblyope = df_without_one_amblyope[time_series_df['id'] != amblyope_ids[2]].copy()\n",
      "C:\\Users\\WERPELGA\\AppData\\Local\\Temp\\ipykernel_22744\\2895359124.py:9: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_modified = df_without_one_amblyope[time_series_df['id'] != amblyope_ids[3]].copy()\n"
     ]
    }
   ],
   "source": [
    "# Find all unique IDs starting with 'A'\n",
    "amblyope_ids = time_series_df['id'].unique()\n",
    "amblyope_ids = [id_ for id_ in amblyope_ids if id_.startswith('A1')]\n",
    "\n",
    "# Create a new DataFrame without the selected amblyope\n",
    "df_without_one_amblyope = time_series_df[time_series_df['id'] != amblyope_ids[0]].copy()\n",
    "df_without_one_amblyope = df_without_one_amblyope[time_series_df['id'] != amblyope_ids[1]].copy()\n",
    "df_without_one_amblyope = df_without_one_amblyope[time_series_df['id'] != amblyope_ids[2]].copy()\n",
    "df_modified = df_without_one_amblyope[time_series_df['id'] != amblyope_ids[3]].copy()\n",
    "\n",
    "# Extract unique IDs from the modified DataFrame\n",
    "unique_ids = df_modified['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the IDs\n",
    "np.random.seed()  # For reproducibility\n",
    "shuffled_ids = np.random.permutation(unique_ids)\n",
    "\n",
    "# Create a mapping from original IDs to shuffled IDs\n",
    "id_mapping = dict(zip(unique_ids, shuffled_ids))\n",
    "\n",
    "# Replace the IDs in the DataFrame with the shuffled IDs\n",
    "df_modified['id'] = df_modified['id'].map(id_mapping)\n",
    "\n",
    "# Select only the required columns\n",
    "df_modified = df_modified[['id', 'time', 'O1', 'Oz', 'O2']]\n",
    "\n",
    "#Reset indexes\n",
    "df_modified.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.03s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:08<00:00,  2.73s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:09<00:00,  3.27s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:07<00:00,  2.48s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:07<00:00,  2.65s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:09<00:00,  3.19s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:10<00:00,  3.38s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.75s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.15s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.72s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.77s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.75s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.72s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.73s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.79s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.74s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.68s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.71s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.70s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:04<00:00,  1.66s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.99s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.03s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.93s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.05s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.05s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.02s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.80s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.99s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.03s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.01s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.09s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.06s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.05s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.04s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:05<00:00,  1.99s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.01s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.03s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.07s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:06<00:00,  2.06s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:07<00:00,  2.47s/it]\n",
      "Feature Extraction: 100%|██████████| 3/3 [00:07<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [   1  413  421  429  841  849  857 1269 1277] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'classifier__max_depth': None, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 100}\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.33      0.36         6\n",
      "           1       0.43      0.50      0.46         6\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.41      0.42      0.41        12\n",
      "weighted avg       0.41      0.42      0.41        12\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      "[[2 4]\n",
      " [3 3]]\n",
      "Important features for Random Forest:\n",
      "                                        Feature  Importance\n",
      "42  Oz__fft_coefficient__attr_\"angle\"__coeff_41    0.083634\n",
      "9   O1__fft_coefficient__attr_\"angle\"__coeff_65    0.051311\n",
      "15  O2__fft_coefficient__attr_\"angle\"__coeff_15    0.047280\n",
      "29   O2__fft_coefficient__attr_\"real\"__coeff_90    0.045706\n",
      "36   Oz__fft_coefficient__attr_\"real\"__coeff_28    0.045236\n",
      "27   O2__fft_coefficient__attr_\"imag\"__coeff_86    0.043840\n",
      "4     O1__fft_coefficient__attr_\"abs\"__coeff_23    0.038752\n",
      "24  O2__fft_coefficient__attr_\"angle\"__coeff_41    0.035402\n",
      "47  Oz__fft_coefficient__attr_\"angle\"__coeff_82    0.032700\n",
      "30   O2__fft_coefficient__attr_\"real\"__coeff_98    0.031091\n",
      "32    Oz__fft_coefficient__attr_\"real\"__coeff_8    0.027124\n",
      "43   Oz__fft_coefficient__attr_\"real\"__coeff_47    0.027108\n",
      "48   Oz__fft_coefficient__attr_\"real\"__coeff_90    0.026595\n",
      "35    Oz__fft_coefficient__attr_\"abs\"__coeff_27    0.024255\n",
      "44  Oz__fft_coefficient__attr_\"angle\"__coeff_56    0.024243\n",
      "1     O1__fft_coefficient__attr_\"real\"__coeff_7    0.023208\n",
      "31  O2__fft_coefficient__attr_\"angle\"__coeff_98    0.022750\n",
      "23   O2__fft_coefficient__attr_\"real\"__coeff_41    0.021114\n",
      "46  Oz__fft_coefficient__attr_\"angle\"__coeff_66    0.020185\n",
      "3     O1__fft_coefficient__attr_\"abs\"__coeff_20    0.018674\n",
      "5    O1__fft_coefficient__attr_\"real\"__coeff_27    0.018378\n",
      "12   O1__fft_coefficient__attr_\"real\"__coeff_93    0.017792\n",
      "8    O1__fft_coefficient__attr_\"real\"__coeff_47    0.017350\n",
      "2     O1__fft_coefficient__attr_\"real\"__coeff_8    0.017017\n",
      "45  Oz__fft_coefficient__attr_\"angle\"__coeff_65    0.016011\n",
      "33    Oz__fft_coefficient__attr_\"abs\"__coeff_20    0.015929\n",
      "7   O1__fft_coefficient__attr_\"angle\"__coeff_35    0.015826\n",
      "49  Oz__fft_coefficient__attr_\"angle\"__coeff_93    0.015311\n",
      "16    O2__fft_coefficient__attr_\"abs\"__coeff_16    0.014707\n",
      "34    Oz__fft_coefficient__attr_\"abs\"__coeff_26    0.014440\n",
      "37   Oz__fft_coefficient__attr_\"imag\"__coeff_35    0.013515\n",
      "28  O2__fft_coefficient__attr_\"angle\"__coeff_86    0.012833\n",
      "11   O1__fft_coefficient__attr_\"imag\"__coeff_83    0.012433\n",
      "41   Oz__fft_coefficient__attr_\"real\"__coeff_41    0.011805\n",
      "17    O2__fft_coefficient__attr_\"abs\"__coeff_20    0.010067\n",
      "18   O2__fft_coefficient__attr_\"imag\"__coeff_24    0.009704\n",
      "25    O2__fft_coefficient__attr_\"abs\"__coeff_58    0.009652\n",
      "20   O2__fft_coefficient__attr_\"real\"__coeff_28    0.009534\n",
      "39   Oz__fft_coefficient__attr_\"real\"__coeff_40    0.009532\n",
      "19   O2__fft_coefficient__attr_\"real\"__coeff_27    0.009527\n",
      "38  Oz__fft_coefficient__attr_\"angle\"__coeff_35    0.006336\n",
      "6    O1__fft_coefficient__attr_\"real\"__coeff_28    0.006023\n",
      "13    O1__fft_coefficient__attr_\"abs\"__coeff_99    0.005412\n",
      "22  O2__fft_coefficient__attr_\"angle\"__coeff_35    0.004767\n",
      "40   Oz__fft_coefficient__attr_\"imag\"__coeff_40    0.004512\n",
      "21   O2__fft_coefficient__attr_\"imag\"__coeff_35    0.003306\n",
      "10   O1__fft_coefficient__attr_\"real\"__coeff_68    0.003178\n",
      "0     O1__fft_coefficient__attr_\"real\"__coeff_6    0.001687\n",
      "14   O2__fft_coefficient__attr_\"imag\"__coeff_15    0.001661\n",
      "26   O2__fft_coefficient__attr_\"imag\"__coeff_69    0.001547\n",
      "Model saved to ml_models/Random Forest_accuracy_41.67%.pkl\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [   1  413  421  429  841  849  857 1269 1277] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200}\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55         6\n",
      "           1       0.57      0.67      0.62         6\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.59      0.58      0.58        12\n",
      "weighted avg       0.59      0.58      0.58        12\n",
      "\n",
      "Confusion Matrix for Gradient Boosting:\n",
      "[[3 3]\n",
      " [2 4]]\n",
      "Important features for Gradient Boosting:\n",
      "                                        Feature    Importance\n",
      "42  Oz__fft_coefficient__attr_\"angle\"__coeff_41  3.844763e-01\n",
      "17    O2__fft_coefficient__attr_\"abs\"__coeff_20  1.566392e-01\n",
      "48   Oz__fft_coefficient__attr_\"real\"__coeff_90  1.481339e-01\n",
      "22  O2__fft_coefficient__attr_\"angle\"__coeff_35  8.609287e-02\n",
      "45  Oz__fft_coefficient__attr_\"angle\"__coeff_65  5.857304e-02\n",
      "30   O2__fft_coefficient__attr_\"real\"__coeff_98  5.351902e-02\n",
      "9   O1__fft_coefficient__attr_\"angle\"__coeff_65  4.810742e-02\n",
      "29   O2__fft_coefficient__attr_\"real\"__coeff_90  2.400553e-02\n",
      "47  Oz__fft_coefficient__attr_\"angle\"__coeff_82  1.817799e-02\n",
      "44  Oz__fft_coefficient__attr_\"angle\"__coeff_56  1.490618e-02\n",
      "4     O1__fft_coefficient__attr_\"abs\"__coeff_23  5.137617e-03\n",
      "14   O2__fft_coefficient__attr_\"imag\"__coeff_15  1.884020e-03\n",
      "35    Oz__fft_coefficient__attr_\"abs\"__coeff_27  3.371435e-04\n",
      "41   Oz__fft_coefficient__attr_\"real\"__coeff_41  1.698583e-06\n",
      "2     O1__fft_coefficient__attr_\"real\"__coeff_8  1.664561e-06\n",
      "7   O1__fft_coefficient__attr_\"angle\"__coeff_35  1.631230e-06\n",
      "32    Oz__fft_coefficient__attr_\"real\"__coeff_8  1.097016e-06\n",
      "19   O2__fft_coefficient__attr_\"real\"__coeff_27  1.002182e-06\n",
      "39   Oz__fft_coefficient__attr_\"real\"__coeff_40  9.820972e-07\n",
      "12   O1__fft_coefficient__attr_\"real\"__coeff_93  4.867905e-07\n",
      "37   Oz__fft_coefficient__attr_\"imag\"__coeff_35  4.770225e-07\n",
      "15  O2__fft_coefficient__attr_\"angle\"__coeff_15  4.674538e-07\n",
      "3     O1__fft_coefficient__attr_\"abs\"__coeff_20  1.374311e-07\n",
      "21   O2__fft_coefficient__attr_\"imag\"__coeff_35  1.252625e-07\n",
      "16    O2__fft_coefficient__attr_\"abs\"__coeff_16  1.603375e-08\n",
      "0     O1__fft_coefficient__attr_\"real\"__coeff_6  1.123690e-18\n",
      "38  Oz__fft_coefficient__attr_\"angle\"__coeff_35  0.000000e+00\n",
      "40   Oz__fft_coefficient__attr_\"imag\"__coeff_40  0.000000e+00\n",
      "36   Oz__fft_coefficient__attr_\"real\"__coeff_28  0.000000e+00\n",
      "33    Oz__fft_coefficient__attr_\"abs\"__coeff_20  0.000000e+00\n",
      "43   Oz__fft_coefficient__attr_\"real\"__coeff_47  0.000000e+00\n",
      "46  Oz__fft_coefficient__attr_\"angle\"__coeff_66  0.000000e+00\n",
      "34    Oz__fft_coefficient__attr_\"abs\"__coeff_26  0.000000e+00\n",
      "25    O2__fft_coefficient__attr_\"abs\"__coeff_58  0.000000e+00\n",
      "31  O2__fft_coefficient__attr_\"angle\"__coeff_98  0.000000e+00\n",
      "28  O2__fft_coefficient__attr_\"angle\"__coeff_86  0.000000e+00\n",
      "27   O2__fft_coefficient__attr_\"imag\"__coeff_86  0.000000e+00\n",
      "26   O2__fft_coefficient__attr_\"imag\"__coeff_69  0.000000e+00\n",
      "1     O1__fft_coefficient__attr_\"real\"__coeff_7  0.000000e+00\n",
      "24  O2__fft_coefficient__attr_\"angle\"__coeff_41  0.000000e+00\n",
      "23   O2__fft_coefficient__attr_\"real\"__coeff_41  0.000000e+00\n",
      "20   O2__fft_coefficient__attr_\"real\"__coeff_28  0.000000e+00\n",
      "18   O2__fft_coefficient__attr_\"imag\"__coeff_24  0.000000e+00\n",
      "13    O1__fft_coefficient__attr_\"abs\"__coeff_99  0.000000e+00\n",
      "11   O1__fft_coefficient__attr_\"imag\"__coeff_83  0.000000e+00\n",
      "10   O1__fft_coefficient__attr_\"real\"__coeff_68  0.000000e+00\n",
      "8    O1__fft_coefficient__attr_\"real\"__coeff_47  0.000000e+00\n",
      "6    O1__fft_coefficient__attr_\"real\"__coeff_28  0.000000e+00\n",
      "5    O1__fft_coefficient__attr_\"real\"__coeff_27  0.000000e+00\n",
      "49  Oz__fft_coefficient__attr_\"angle\"__coeff_93  0.000000e+00\n",
      "Model saved to ml_models/Gradient Boosting_accuracy_58.33%.pkl\n",
      "\n",
      "Training and evaluating Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [   1  413  421  429  841  849  857 1269 1277] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Neural Network: {'classifier__activation': 'relu', 'classifier__alpha': 0.0001, 'classifier__hidden_layer_sizes': (50, 50), 'classifier__learning_rate': 'adaptive', 'classifier__solver': 'sgd'}\n",
      "Classification Report for Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50         6\n",
      "           1       0.50      0.50      0.50         6\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.50      0.50      0.50        12\n",
      "weighted avg       0.50      0.50      0.50        12\n",
      "\n",
      "Confusion Matrix for Neural Network:\n",
      "[[3 3]\n",
      " [3 3]]\n",
      "Neural Network does not provide feature importances directly.\n",
      "Model saved to ml_models/Neural Network_accuracy_50.0%.pkl\n",
      "\n",
      "Training and evaluating Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [   1  413  421  429  841  849  857 1269 1277] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60         6\n",
      "           1       0.62      0.83      0.71         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.69      0.67      0.66        12\n",
      "weighted avg       0.69      0.67      0.66        12\n",
      "\n",
      "Confusion Matrix for Logistic Regression:\n",
      "[[3 3]\n",
      " [1 5]]\n",
      "Logistic Regression does not provide feature importances directly.\n",
      "Model saved to ml_models/Logistic Regression_accuracy_66.67%.pkl\n",
      "\n",
      "Training and evaluating Support Vector Machine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [   1  413  421  429  841  849  857 1269 1277] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Support Vector Machine: {'classifier__C': 10, 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'poly'}\n",
      "Classification Report for Support Vector Machine:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.67      0.67      0.67        12\n",
      "weighted avg       0.67      0.67      0.67        12\n",
      "\n",
      "Confusion Matrix for Support Vector Machine:\n",
      "[[4 2]\n",
      " [2 4]]\n",
      "Support Vector Machine does not provide feature importances directly.\n",
      "Model saved to ml_models/Support Vector Machine_accuracy_66.67%.pkl\n",
      "\n",
      "Training and evaluating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [   1  413  421  429  841  849  857 1269 1277] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:11:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'classifier__colsample_bytree': 0.5, 'classifier__learning_rate': 0.2, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 0.7}\n",
      "Classification Report for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55         6\n",
      "           1       0.57      0.67      0.62         6\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.59      0.58      0.58        12\n",
      "weighted avg       0.59      0.58      0.58        12\n",
      "\n",
      "Confusion Matrix for XGBoost:\n",
      "[[3 3]\n",
      " [2 4]]\n",
      "Important features for XGBoost:\n",
      "                                        Feature  Importance\n",
      "25    O2__fft_coefficient__attr_\"abs\"__coeff_58    0.189682\n",
      "33    Oz__fft_coefficient__attr_\"abs\"__coeff_20    0.108955\n",
      "46  Oz__fft_coefficient__attr_\"angle\"__coeff_66    0.072355\n",
      "48   Oz__fft_coefficient__attr_\"real\"__coeff_90    0.063771\n",
      "15  O2__fft_coefficient__attr_\"angle\"__coeff_15    0.058158\n",
      "37   Oz__fft_coefficient__attr_\"imag\"__coeff_35    0.056821\n",
      "2     O1__fft_coefficient__attr_\"real\"__coeff_8    0.044502\n",
      "42  Oz__fft_coefficient__attr_\"angle\"__coeff_41    0.037651\n",
      "34    Oz__fft_coefficient__attr_\"abs\"__coeff_26    0.034259\n",
      "9   O1__fft_coefficient__attr_\"angle\"__coeff_65    0.033951\n",
      "16    O2__fft_coefficient__attr_\"abs\"__coeff_16    0.026828\n",
      "27   O2__fft_coefficient__attr_\"imag\"__coeff_86    0.025967\n",
      "49  Oz__fft_coefficient__attr_\"angle\"__coeff_93    0.025636\n",
      "47  Oz__fft_coefficient__attr_\"angle\"__coeff_82    0.024660\n",
      "18   O2__fft_coefficient__attr_\"imag\"__coeff_24    0.024293\n",
      "28  O2__fft_coefficient__attr_\"angle\"__coeff_86    0.020424\n",
      "32    Oz__fft_coefficient__attr_\"real\"__coeff_8    0.018478\n",
      "39   Oz__fft_coefficient__attr_\"real\"__coeff_40    0.016192\n",
      "17    O2__fft_coefficient__attr_\"abs\"__coeff_20    0.014925\n",
      "8    O1__fft_coefficient__attr_\"real\"__coeff_47    0.014593\n",
      "11   O1__fft_coefficient__attr_\"imag\"__coeff_83    0.013009\n",
      "12   O1__fft_coefficient__attr_\"real\"__coeff_93    0.012604\n",
      "43   Oz__fft_coefficient__attr_\"real\"__coeff_47    0.012557\n",
      "44  Oz__fft_coefficient__attr_\"angle\"__coeff_56    0.010169\n",
      "29   O2__fft_coefficient__attr_\"real\"__coeff_90    0.009868\n",
      "31  O2__fft_coefficient__attr_\"angle\"__coeff_98    0.009349\n",
      "41   Oz__fft_coefficient__attr_\"real\"__coeff_41    0.008867\n",
      "5    O1__fft_coefficient__attr_\"real\"__coeff_27    0.006859\n",
      "19   O2__fft_coefficient__attr_\"real\"__coeff_27    0.003636\n",
      "3     O1__fft_coefficient__attr_\"abs\"__coeff_20    0.000981\n",
      "36   Oz__fft_coefficient__attr_\"real\"__coeff_28    0.000000\n",
      "45  Oz__fft_coefficient__attr_\"angle\"__coeff_65    0.000000\n",
      "40   Oz__fft_coefficient__attr_\"imag\"__coeff_40    0.000000\n",
      "4     O1__fft_coefficient__attr_\"abs\"__coeff_23    0.000000\n",
      "38  Oz__fft_coefficient__attr_\"angle\"__coeff_35    0.000000\n",
      "6    O1__fft_coefficient__attr_\"real\"__coeff_28    0.000000\n",
      "13    O1__fft_coefficient__attr_\"abs\"__coeff_99    0.000000\n",
      "35    Oz__fft_coefficient__attr_\"abs\"__coeff_27    0.000000\n",
      "7   O1__fft_coefficient__attr_\"angle\"__coeff_35    0.000000\n",
      "14   O2__fft_coefficient__attr_\"imag\"__coeff_15    0.000000\n",
      "30   O2__fft_coefficient__attr_\"real\"__coeff_98    0.000000\n",
      "10   O1__fft_coefficient__attr_\"real\"__coeff_68    0.000000\n",
      "26   O2__fft_coefficient__attr_\"imag\"__coeff_69    0.000000\n",
      "1     O1__fft_coefficient__attr_\"real\"__coeff_7    0.000000\n",
      "24  O2__fft_coefficient__attr_\"angle\"__coeff_41    0.000000\n",
      "23   O2__fft_coefficient__attr_\"real\"__coeff_41    0.000000\n",
      "22  O2__fft_coefficient__attr_\"angle\"__coeff_35    0.000000\n",
      "21   O2__fft_coefficient__attr_\"imag\"__coeff_35    0.000000\n",
      "20   O2__fft_coefficient__attr_\"real\"__coeff_28    0.000000\n",
      "0     O1__fft_coefficient__attr_\"real\"__coeff_6    0.000000\n",
      "Model saved to ml_models/XGBoost_accuracy_58.33%.pkl\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import EfficientFCParameters, MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "# 1. Create a Custom fc_parameters Dictionary\n",
    "# Load EfficientFCParameters\n",
    "efficient_fc_parameters = EfficientFCParameters()\n",
    "\n",
    "# Create custom fc_parameters with the specified feature calculators\n",
    "custom_fc_parameters = {\n",
    "    'fft_coefficient': [\n",
    "        {'coeff': coeff, 'attr': attr}\n",
    "        for coeff in range(100)  # Adjust the range as needed\n",
    "        for attr in ['real', 'imag', 'abs', 'angle']\n",
    "    ],\n",
    "    'ar_coefficient': [\n",
    "        {'k': 10, 'coeff': coeff} for coeff in range(1, 11)\n",
    "    ],\n",
    "    'augmented_dickey_fuller': [\n",
    "        {'attr': 'teststat'},\n",
    "        {'attr': 'pvalue'},\n",
    "        {'attr': 'usedlag'}\n",
    "    ],\n",
    "    'large_standard_deviation': [\n",
    "        {'r': 0.2}\n",
    "    ],\n",
    "    'number_peaks': [\n",
    "        {'n': n} for n in [1, 5]\n",
    "    ],\n",
    "    'fourier_entropy': [\n",
    "        {'bins': 10}\n",
    "    ],\n",
    "    'ratio_beyond_r_sigma': [\n",
    "        {'r': 1}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Append MinimalFCParameters to custom_fc_parameters\n",
    "minimal_fc_parameters = MinimalFCParameters()\n",
    "custom_fc_parameters.update(minimal_fc_parameters)\n",
    "\n",
    "# 2. Define the function to process data in chunks using the custom fc_parameters\n",
    "def process_in_chunks(df_modified, N, fc_parameters):\n",
    "    # Get unique IDs\n",
    "    unique_ids = df_modified['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the IDs in the current chunk\n",
    "        chunk_df = df_modified[df_modified['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk using the custom fc_parameters\n",
    "        extracted_features_chunk = extract_features(\n",
    "            chunk_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=fc_parameters,\n",
    "            n_jobs=4,\n",
    "        )\n",
    "        \n",
    "        # Impute missing values in the extracted features\n",
    "        impute(extracted_features_chunk)\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features_chunk)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_df, extracted_features_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# 3. Set the chunk size N (adjust based on your memory constraints)\n",
    "N = 1  # Adjust based on your memory constraints\n",
    "\n",
    "# 4. Extract features using the process_in_chunks function with custom_fc_parameters\n",
    "extracted_features = process_in_chunks(df_modified, N, custom_fc_parameters)\n",
    "\n",
    "# 5. Clean the extracted features\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# 6. Ensure that the labels are aligned with the extracted features\n",
    "labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# 7. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_features_clean,\n",
    "    labels_aligned,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=labels_aligned\n",
    ")\n",
    "\n",
    "# 8. Define classifiers and their parameter grids\n",
    "classifiers = {\n",
    "    'Random Forest': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('selector', SelectKBest(f_classif, k=50)),\n",
    "            ('classifier', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [100, 200, 500],\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('selector', SelectKBest(f_classif, k=50)),\n",
    "            ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1],\n",
    "            'classifier__max_depth': [3, 5],\n",
    "            'classifier__min_samples_split': [2, 5],\n",
    "        }\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=50)),\n",
    "            ('classifier', MLPClassifier(random_state=42, max_iter=1000))\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'classifier__activation': ['tanh', 'relu'],\n",
    "            'classifier__solver': ['adam', 'sgd'],\n",
    "            'classifier__alpha': [0.0001, 0.001],\n",
    "            'classifier__learning_rate': ['constant', 'adaptive'],\n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=50)),\n",
    "            ('classifier', LogisticRegression(random_state=42, max_iter=5000))\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'classifier__penalty': ['l1', 'l2'],\n",
    "            'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "            'classifier__solver': ['liblinear'],\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=50)),\n",
    "            ('classifier', SVC(random_state=42))\n",
    "        ]),\n",
    "        'param_grid': [\n",
    "            {\n",
    "                'classifier__kernel': ['linear'],\n",
    "                'classifier__C': [0.1, 1, 10, 100]\n",
    "            },\n",
    "            {\n",
    "                'classifier__kernel': ['rbf'],\n",
    "                'classifier__C': [0.1, 1, 10, 100],\n",
    "                'classifier__gamma': ['scale', 'auto']\n",
    "            },\n",
    "            {\n",
    "                'classifier__kernel': ['poly'],\n",
    "                'classifier__C': [0.1, 1, 10],\n",
    "                'classifier__degree': [2, 3],\n",
    "                'classifier__gamma': ['scale', 'auto']\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('selector', SelectKBest(f_classif, k=50)),\n",
    "            ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [3, 5, 7],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'classifier__subsample': [0.5, 0.7, 1.0],\n",
    "            'classifier__colsample_bytree': [0.5, 0.7, 1.0],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# 9. Create the directory 'ml_models' if it doesn't exist\n",
    "output_dir = 'ml_models'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# 9. Loop through each classifier, perform grid search, and evaluate\n",
    "for name, classifier_info in classifiers.items():\n",
    "    print(f\"\\nTraining and evaluating {name}...\")\n",
    "    pipeline = classifier_info['pipeline']\n",
    "    param_grid = classifier_info['param_grid']\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_percentage = round(accuracy * 100, 2)\n",
    "    \n",
    "    # Print the best parameters found by GridSearchCV\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion Matrix for {name}:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # For models that provide feature importances, display them\n",
    "    classifier_step = best_pipeline.named_steps['classifier']\n",
    "    selector_step = best_pipeline.named_steps['selector']\n",
    "    selected_feature_indices = selector_step.get_support(indices=True)\n",
    "    selected_feature_names = extracted_features_clean.columns[selected_feature_indices]\n",
    "    \n",
    "    if hasattr(classifier_step, 'feature_importances_'):\n",
    "        important_features = pd.DataFrame({\n",
    "            'Feature': selected_feature_names,\n",
    "            'Importance': classifier_step.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(f\"Important features for {name}:\")\n",
    "        print(important_features)\n",
    "    elif hasattr(classifier_step, 'coef_'):\n",
    "        # For linear models like SVM with linear kernel\n",
    "        if hasattr(classifier_step, 'kernel') and classifier_step.kernel == 'linear':\n",
    "            importance = np.abs(classifier_step.coef_[0])\n",
    "            important_features = pd.DataFrame({\n",
    "                'Feature': selected_feature_names,\n",
    "                'Importance': importance\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            print(f\"Important features for {name}:\")\n",
    "            print(important_features)\n",
    "        elif isinstance(classifier_step, MLPClassifier):\n",
    "            # For Neural Network, feature importances are not directly available\n",
    "            print(f\"{name} does not provide feature importances directly.\")\n",
    "        else:\n",
    "            print(f\"{name} does not provide feature importances directly.\")\n",
    "    else:\n",
    "        print(f\"{name} does not provide feature importances directly.\")\n",
    "    \n",
    "     # Save the model to a .pkl file\n",
    "    model_filename = f\"{output_dir}/{name}_accuracy_{accuracy_percentage}%.pkl\"\n",
    "    with open(model_filename, 'wb') as file:\n",
    "        pickle.dump(best_pipeline, file)\n",
    "    print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
