{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mne scipy\n",
    "#!pip install pandas numpy openpyxl\n",
    "#!pip install tsfresh\n",
    "#!pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import mne\n",
    "\n",
    "def process_all_eeg_data() -> dict:\n",
    "    \"\"\"\n",
    "    Process all .bdf EEG files in the current directory, applying filters and extracting data from\n",
    "    channels A15 (O1), A16 (Oz), and A17 (O2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing processed EEG data and header information for each file.\n",
    "    \"\"\"\n",
    "    # Get a list of all .bdf files in the current directory\n",
    "    files = [f for f in os.listdir('.') if f.endswith('.bdf')]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No BDF files found in the current directory\")\n",
    "    \n",
    "    # Initialize the results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Loop over each file\n",
    "    for filename in files:\n",
    "        full_file_path = os.path.join(os.getcwd(), filename)\n",
    "        \n",
    "        # Read the raw EEG data using MNE\n",
    "        raw = mne.io.read_raw_bdf(full_file_path, preload=True)\n",
    "        hdr = raw.info\n",
    "        \n",
    "        # Select data from channels A15 (O1), A16 (Oz), and A17 (O2)\n",
    "        channels_select = ['A15', 'A16', 'A17']\n",
    "        missing_channels = [ch for ch in channels_select if ch not in hdr['ch_names']]\n",
    "        if missing_channels:\n",
    "            raise ValueError(f\"Selected channels {missing_channels} not found in the data\")\n",
    "        \n",
    "        channel_indices = [hdr['ch_names'].index(ch) for ch in channels_select]\n",
    "        EEG_data = raw.get_data(picks=channel_indices).T  # Shape: (n_samples, n_channels)\n",
    "        \n",
    "        # Filter EEG Data\n",
    "        Fs = hdr['sfreq']  # Sampling frequency\n",
    "        \n",
    "        # Bandpass filter parameters (2 to 80 Hz)\n",
    "        Fc_BP = [2, 80]  # Bandpass frequency range\n",
    "        Wn_BP = [f / (Fs / 2) for f in Fc_BP]  # Normalize by Nyquist frequency\n",
    "        \n",
    "        # Create and apply bandpass filter (6th order zero-phase Butterworth IIR)\n",
    "        B_BP, A_BP = signal.butter(3, Wn_BP, btype='bandpass')\n",
    "        EEG_filtered_BP = signal.filtfilt(B_BP, A_BP, EEG_data, axis=0)\n",
    "        \n",
    "        # Band stop filter parameters (48 to 52 Hz)\n",
    "        Fc_BS = [48, 52]  # Band stop frequency range\n",
    "        Wn_BS = [f / (Fs / 2) for f in Fc_BS]  # Normalize by Nyquist frequency\n",
    "        \n",
    "        # Create and apply band stop filter (6th order zero-phase Butterworth IIR)\n",
    "        B_BS, A_BS = signal.butter(3, Wn_BS, btype='bandstop')\n",
    "        EEG_filtered = signal.filtfilt(B_BS, A_BS, EEG_filtered_BP, axis=0)\n",
    "        \n",
    "        # Extract prefix before underscore from the filename\n",
    "        underscore_index = filename.find('_')\n",
    "        if underscore_index == -1:\n",
    "            raise ValueError(f\"Filename format error, no underscore found in {filename}\")\n",
    "        key = filename[:underscore_index]\n",
    "        \n",
    "        # Store results in the dictionary\n",
    "        results[key] = {\n",
    "            'data': EEG_filtered,      # Filtered data for channels A15, A16, A17\n",
    "            'channels': channels_select,  # List of channel names\n",
    "            'header': hdr\n",
    "        }\n",
    "        \n",
    "        # Display a message indicating successful processing\n",
    "        print(f\"Data for file {filename} processed successfully\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A1_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 739327  =      0.000 ...   361.000 secs...\n",
      "Data for file A1_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A3_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 757759  =      0.000 ...   370.000 secs...\n",
      "Data for file A3_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A4_Full_Block.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 782335  =      0.000 ...   382.000 secs...\n",
      "Data for file A4_Full_Block.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A6_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A6_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A7_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A7_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A8_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A8_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\A9_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file A9_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C11_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C11_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C12_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C12_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C13_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C13_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C14_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C14_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C15_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C15_Alpha.bdf processed successfully\n",
      "Extracting EDF parameters from c:\\Users\\WERPELGA\\OneDrive - Danone\\Desktop\\UoA\\2024.1&2\\Python Gabe\\C1_Alpha.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 81919  =      0.000 ...    40.000 secs...\n",
      "Data for file C1_Alpha.bdf processed successfully\n"
     ]
    }
   ],
   "source": [
    "results = process_all_eeg_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def segment_eeg_data_new(results: dict, cohort_file: str = 'Cohort.xlsx') -> dict:\n",
    "    \"\"\"\n",
    "    Segments EEG data into predefined sections (EC, EO, LC, RC, DEC, NDEC) based on cohort information,\n",
    "    removing the first 2 seconds from each section.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        Dictionary containing the raw EEG data and header information for each key (participant).\n",
    "    cohort_file : str, optional\n",
    "        Path to the Excel file containing cohort information (default is 'Cohort.xlsx').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing segmented EEG data for each participant.\n",
    "    \"\"\"\n",
    "    # Read the cohort information from an Excel file\n",
    "    cohort_table = pd.read_excel(cohort_file)\n",
    "    # Segment Duration (in seconds)\n",
    "    segment_duration = 10  # Original segment duration in seconds\n",
    "    skip_duration = 2      # Duration to skip at the start of each segment (2 seconds)\n",
    "\n",
    "    # Initialize the segmented results dictionary\n",
    "    segmented_data = {}\n",
    "\n",
    "    # Iterate through each key in the results dictionary\n",
    "    for key, result in results.items():\n",
    "        data = result['data']  # Data shape: (n_samples, n_channels)\n",
    "        hdr = result['header']\n",
    "\n",
    "        # Find the matching row in the cohort table\n",
    "        cohort_row = cohort_table[cohort_table['Cohort'] == key]\n",
    "        \n",
    "        if cohort_row.empty:\n",
    "            raise ValueError(f\"Cohort information not found for {key}\")\n",
    "\n",
    "        # Define the sample rate and calculate sample counts\n",
    "        Fs = hdr['sfreq']  # Sampling frequency\n",
    "        samples_per_segment = int(segment_duration * Fs)\n",
    "        samples_to_skip = int(skip_duration * Fs)\n",
    "        effective_samples_per_segment = samples_per_segment - samples_to_skip\n",
    "        n_channels = data.shape[1]  # Number of channels (should be 3: O1, Oz, O2)\n",
    "\n",
    "        # Initialize segments with zeros\n",
    "        EC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        EO = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        LC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        RC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        DEC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "        NDEC = np.zeros((effective_samples_per_segment, n_channels))\n",
    "\n",
    "        # Fill segments with data if available, skipping the first 2 seconds\n",
    "        # EC segment\n",
    "        segment_start = 0\n",
    "        segment_end = samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            EC = data[segment_start + samples_to_skip : 0, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for EC segment in {key}\")\n",
    "\n",
    "        # EO segment\n",
    "        segment_start = samples_per_segment\n",
    "        segment_end = 2 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            EO = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for EO segment in {key}\")\n",
    "\n",
    "        # LC segment\n",
    "        segment_start = 2 * samples_per_segment\n",
    "        segment_end = 3 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            LC = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for LC segment in {key}\")\n",
    "\n",
    "        # RC segment\n",
    "        segment_start = 3 * samples_per_segment\n",
    "        segment_end = 4 * samples_per_segment\n",
    "        if data.shape[0] >= segment_end:\n",
    "            RC = data[segment_start + samples_to_skip : segment_end, :]\n",
    "        else:\n",
    "            print(f\"Not enough data for RC segment in {key}\")\n",
    "\n",
    "        # Apply conditions based on cohort table\n",
    "        if cohort_row['LC'].values[0] == 'DEC':\n",
    "            # Assign 'DEC' to LC and 'NDEC' to RC\n",
    "            DEC = LC\n",
    "            NDEC = RC\n",
    "        elif cohort_row['RC'].values[0] == 'DEC':\n",
    "            # Assign 'DEC' to RC and 'NDEC' to LC\n",
    "            DEC = RC\n",
    "            NDEC = LC\n",
    "        else:\n",
    "            # If neither LC nor RC is 'DEC', assign NDEC accordingly\n",
    "            NDEC = LC\n",
    "            # Optionally handle cases where DEC is not specified\n",
    "            DEC = RC  # Or set DEC to zeros if appropriate\n",
    "\n",
    "        # Store the segmented data and 'LinesDifference' in the results dictionary\n",
    "        segmented_data[key] = {\n",
    "            'header': hdr,\n",
    "            'EC': EC,\n",
    "            'EO': EO,\n",
    "            'DEC': DEC,\n",
    "            'NDEC': NDEC,\n",
    "            'LinesDifference': cohort_row['LinesDifference'].values[0]\n",
    "        }\n",
    "\n",
    "    return segmented_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data = segment_eeg_data_new(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def prepare_time_series_by_section(segmented_data, cohort_table):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame suitable for tsfresh from segmented EEG data for all sections (EC, EO, DEC, NDEC).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    segmented_data : dict\n",
    "        The dictionary containing segmented EEG data for each participant.\n",
    "    cohort_table : pd.DataFrame\n",
    "        DataFrame containing cohort information (including labels for Amblyopia/Control).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame, pd.Series\n",
    "        A DataFrame where each row represents a time-series sample with columns 'id', 'time', 'O1', 'Oz', 'O2',\n",
    "        and a Series with group labels indexed by 'id'.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Loop through each participant's data\n",
    "    for key, value in segmented_data.items():\n",
    "        # Find the matching cohort row\n",
    "        cohort_row = cohort_table[cohort_table['Cohort'] == key]\n",
    "        if cohort_row.empty:\n",
    "            continue\n",
    "\n",
    "        # Assign label based on the first letter of the 'Cohort' column (Amblyopia = 1, Control = 0)\n",
    "        label = 1 if key.startswith('A') else 0\n",
    "\n",
    "        # Get channel names; default to ['O1', 'Oz', 'O2'] if not available\n",
    "        channels = value.get('channels', ['O1', 'Oz', 'O2'])\n",
    "\n",
    "        # For each section (EC, EO, DEC, NDEC)\n",
    "        for section in ['EC', 'EO', 'DEC', 'NDEC']:\n",
    "            section_data = value[section]  # Shape: (n_samples, n_channels)\n",
    "\n",
    "            # Create a DataFrame for this section\n",
    "            n_samples = section_data.shape[0]\n",
    "            df = pd.DataFrame({\n",
    "                'id': f\"{key}_{section}\",\n",
    "                'time': np.arange(n_samples)\n",
    "            })\n",
    "\n",
    "            # Add each channel's data as a column\n",
    "            for idx, channel_name in enumerate(channels):\n",
    "                df[channel_name] = section_data[:, idx]\n",
    "\n",
    "            # Append to data list\n",
    "            data_list.append(df)\n",
    "\n",
    "            # Append label for this 'id' (participant_section)\n",
    "            labels_list.append({'id': f\"{key}_{section}\", 'label': label})\n",
    "\n",
    "    # Concatenate all data into a single DataFrame\n",
    "    time_series_df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "    # Create a labels DataFrame and convert to a Series indexed by 'id'\n",
    "    labels_df = pd.DataFrame(labels_list).drop_duplicates(subset='id')\n",
    "    labels_series = labels_df.set_index('id')['label']\n",
    "\n",
    "    # Return the time-series data and corresponding labels\n",
    "    return time_series_df, labels_series\n",
    "\n",
    "# Load your cohort table (must include 'Cohort' column)\n",
    "cohort_table = pd.read_excel('Cohort.xlsx')\n",
    "\n",
    "# Prepare the time series DataFrame and labels\n",
    "time_series_df, labels = prepare_time_series_by_section(segmented_data, cohort_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired percentage of rows to keep\n",
    "percentage = 0.25\n",
    "\n",
    "# Sample a random subset of rows from the DataFrame\n",
    "time_series_df_sampled = time_series_df.sample(frac=percentage, random_state=42)\n",
    "\n",
    "# Reset the index of the sampled DataFrame\n",
    "time_series_df_sampled.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id   time            O1            Oz            O2\n",
      "0        A9_NDEC   6508  6.183467e-07  2.611819e-06  1.660337e-06\n",
      "1        C13_DEC   1452 -4.879078e-07 -3.953822e-07  1.569165e-06\n",
      "2        C15_DEC   6232  9.608291e-07 -6.445617e-07  7.503135e-07\n",
      "3        A6_NDEC   5108 -4.289662e-06 -2.234704e-06 -6.283902e-06\n",
      "4          A6_EO  14867  1.096299e-05  9.870873e-06  1.238889e-05\n",
      "...          ...    ...           ...           ...           ...\n",
      "159739   A3_NDEC   3651 -1.245478e-05 -8.486548e-06 -7.957843e-06\n",
      "159740  C13_NDEC   4522  3.169404e-06  3.493584e-06  7.299572e-06\n",
      "159741   C15_DEC   7378 -3.286281e-06  7.410397e-08  1.883803e-07\n",
      "159742    A1_DEC   2598 -7.838452e-07 -2.541843e-06 -1.972450e-06\n",
      "159743  C13_NDEC   8794 -6.742923e-07 -2.828545e-06 -4.322975e-06\n",
      "\n",
      "[159744 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "time_series_df = time_series_df_sampled\n",
    "print(time_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save time_series_df as CSV\n",
    "time_series_df.to_csv('time_series_df_full.csv', index=False)\n",
    "\n",
    "# Save labels as CSV\n",
    "labels.to_csv('labels_full.csv', index=False, header=True)\n",
    "\n",
    "# Optionally, save labels as Pickle (preserves Python object types)\n",
    "# labels.to_pickle('labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read time_series_df from CSV\n",
    "# time_series_df = pd.read_csv('time_series_df_full.csv')\n",
    "\n",
    "# # Read labels from CSV\n",
    "# labels = pd.read_csv('labels_full.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.index = time_series_df['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 15/15 [00:26<00:00,  1.74s/it]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:28<00:00,  1.87s/it]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:30<00:00,  2.07s/it]\n",
      "Feature Extraction: 100%|██████████| 14/14 [00:29<00:00,  2.09s/it]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [   0    1    2    3   27   28   29   30   31   45   46   47   48   49\n",
      "   50   51   52   53   54   55   56   57   58   59   60   61   62   63\n",
      "   64   65   68   69   70   71   72   73   74   75   76   77   78   79\n",
      "   80   81   82   83   95  355  659  660  661  732  733  752  753  772\n",
      "  773  774  775  799  800  801  802  803  817  818  819  820  821  822\n",
      "  823  824  825  826  827  828  829  830  831  832  833  834  835  836\n",
      "  837  840  841  842  843  844  845  846  847  848  849  850  851  852\n",
      "  853  854  855  867 1127 1431 1432 1433 1504 1505 1524 1525 1544 1545\n",
      " 1546 1547 1571 1572 1573 1574 1575 1589 1590 1591 1592 1593 1594 1595\n",
      " 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609\n",
      " 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625\n",
      " 1626 1627 1639 1899 2203 2204 2205 2276 2277 2297] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.33      0.40         6\n",
      "           1       0.50      0.67      0.57         6\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.50      0.50      0.49        12\n",
      "weighted avg       0.50      0.50      0.49        12\n",
      "\n",
      "                                             Feature  Importance\n",
      "0                              O2__number_peaks__n_1    0.211177\n",
      "9        O2__permutation_entropy__dimension_7__tau_1    0.170164\n",
      "1                  O2__ar_coefficient__coeff_1__k_10    0.166483\n",
      "6        O2__permutation_entropy__dimension_4__tau_1    0.102123\n",
      "8        O2__permutation_entropy__dimension_6__tau_1    0.092385\n",
      "2  O2__change_quantiles__f_agg_\"mean\"__isabs_True...    0.078651\n",
      "5        O2__permutation_entropy__dimension_3__tau_1    0.078345\n",
      "7        O2__permutation_entropy__dimension_5__tau_1    0.078290\n",
      "4  O2__change_quantiles__f_agg_\"mean\"__isabs_True...    0.022383\n",
      "3  O2__change_quantiles__f_agg_\"var\"__isabs_False...    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "\n",
    "# Define the function to process data in chunks using ComprehensiveFCParameters\n",
    "def process_in_chunks(time_series_df, N):\n",
    "    # Get unique IDs\n",
    "    unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the IDs in the current chunk\n",
    "        chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk using ComprehensiveFCParameters\n",
    "        extracted_features_chunk = extract_features(\n",
    "            chunk_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=ComprehensiveFCParameters()\n",
    "        )\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features_chunk)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_df, extracted_features_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Set the chunk size N (adjust based on your memory constraints)\n",
    "N = 10  # Smaller chunk size to manage memory usage\n",
    "\n",
    "# Extract features using the process_in_chunks function\n",
    "extracted_features = process_in_chunks(time_series_df, N)\n",
    "\n",
    "# Drop any columns with NaN or infinite values\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Ensure that the labels are aligned with the extracted features\n",
    "# Assuming 'labels' is a Series with 'id' as the index\n",
    "labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_features_clean,\n",
    "    labels_aligned,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Select the most important features using ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=10)  # Adjust 'k' as needed\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best estimator from GridSearchCV to predict and evaluate the model\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Identify and display the top selected features with importance\n",
    "selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "important_features = pd.DataFrame({\n",
    "    'Feature': selected_feature_names,\n",
    "    'Importance': best_clf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(important_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Best Parameters Found by GridSearchCV\n",
    "plaintext\n",
    "Copy code\n",
    "Best parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
    "Explanation:\n",
    "\n",
    "max_depth: None: This means there's no limit to how deep each tree in the forest can grow. The nodes will expand until all leaves are pure or until all leaves contain fewer samples than min_samples_split.\n",
    "\n",
    "min_samples_split: 2: This is the minimum number of samples required to split an internal node. A value of 2 is the default and allows the tree to grow as much as possible.\n",
    "\n",
    "n_estimators: 100: The number of trees in the forest is 100. More trees can lead to better performance but also increase computation time.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The grid search determined that the default parameters are optimal within the range you provided. Essentially, the model performs best without restrictions on tree depth and with the default settings for splitting and the number of trees.\n",
    "\n",
    "2. Classification Report\n",
    "plaintext\n",
    "Copy code\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      1.00      0.67         6\n",
    "           1       1.00      0.40      0.57        10\n",
    "\n",
    "    accuracy                           0.62        16\n",
    "   macro avg       0.75      0.70      0.62        16\n",
    "weighted avg       0.81      0.62      0.61        16\n",
    "Metrics Explanation:\n",
    "\n",
    "Support: The number of occurrences of each class in the test set.\n",
    "\n",
    "Class 0: 6 instances.\n",
    "Class 1: 10 instances.\n",
    "Precision: The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "\n",
    "Class 0: 0.50 (50% of the instances predicted as class 0 are actually class 0).\n",
    "Class 1: 1.00 (100% of the instances predicted as class 1 are actually class 1).\n",
    "Recall: The ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "Class 0: 1.00 (100% of actual class 0 instances are correctly identified).\n",
    "Class 1: 0.40 (Only 40% of actual class 1 instances are correctly identified).\n",
    "F1-score: The harmonic mean of precision and recall.\n",
    "\n",
    "Class 0: 0.67.\n",
    "Class 1: 0.57.\n",
    "Accuracy: Overall, 62% of the test set instances are correctly classified.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Class 0 (Control Group or Non-Amblyopia):\n",
    "\n",
    "High Recall (1.00): The model correctly identified all instances of class 0.\n",
    "Low Precision (0.50): Half of the instances predicted as class 0 are actually from class 1 (false positives).\n",
    "Class 1 (Amblyopia Group):\n",
    "\n",
    "High Precision (1.00): All instances predicted as class 1 are correctly from class 1.\n",
    "Low Recall (0.40): The model failed to identify 60% of actual class 1 instances (false negatives).\n",
    "Overall Performance:\n",
    "\n",
    "The model is better at identifying class 0 but struggles to correctly identify all instances of class 1.\n",
    "Accuracy is 62%, which may not be satisfactory depending on the context.\n",
    "Macro Average:\n",
    "Precision: 0.75.\n",
    "Recall: 0.70.\n",
    "F1-score: 0.62.\n",
    "Possible Reasons for the Performance:\n",
    "\n",
    "Class Imbalance: Although the classes are relatively balanced (6 vs. 10), the model may still be biased towards class 0.\n",
    "Small Dataset: With only 16 instances in the test set, the model's performance metrics may not be stable or representative.\n",
    "Overfitting: The model may have overfitted to the training data, especially if the training set is small or if the model is too complex.\n",
    "Feature Selection: Limiting to 10 features may have excluded important predictors.\n",
    "3. Important Features and Their Importances\n",
    "plaintext\n",
    "Copy code\n",
    "                      Feature  Importance\n",
    "4             O1__maximum    0.207673\n",
    "8    O2__absolute_maximum    0.140004\n",
    "7             O2__maximum    0.126442\n",
    "9             O2__minimum    0.125662\n",
    "1  O1__standard_deviation    0.092274\n",
    "5    O1__absolute_maximum    0.088193\n",
    "6             O1__minimum    0.083645\n",
    "3    O1__root_mean_square    0.077596\n",
    "0              O1__median    0.058510\n",
    "2            O1__variance    0.000000\n",
    "Feature Descriptions:\n",
    "\n",
    "Channel O1:\n",
    "\n",
    "O1__maximum: The maximum value of the EEG signal in the O1 channel.\n",
    "O1__absolute_maximum: The largest absolute value in the O1 channel.\n",
    "O1__minimum: The minimum value in the O1 channel.\n",
    "O1__standard_deviation: The standard deviation of the O1 signal.\n",
    "O1__root_mean_square: The RMS value of the O1 signal.\n",
    "O1__median: The median value of the O1 signal.\n",
    "O1__variance: The variance of the O1 signal.\n",
    "Channel O2:\n",
    "\n",
    "O2__absolute_maximum: The largest absolute value in the O2 channel.\n",
    "O2__maximum: The maximum value of the EEG signal in the O2 channel.\n",
    "O2__minimum: The minimum value in the O2 channel.\n",
    "Feature Importances:\n",
    "\n",
    "The importance values indicate the relative contribution of each feature to the model's decision-making.\n",
    "\n",
    "Top Features:\n",
    "\n",
    "O1__maximum (0.2077): Most significant feature.\n",
    "O2__absolute_maximum (0.1400).\n",
    "O2__maximum (0.1264).\n",
    "O2__minimum (0.1257).\n",
    "Zero Importance Feature:\n",
    "\n",
    "O1__variance (0.0000): This feature did not contribute to the model's predictions.\n",
    "Interpretation:\n",
    "\n",
    "The model heavily relies on maximum and minimum amplitude values from channels O1 and O2.\n",
    "Features like standard deviation and root mean square also play a role but are less significant.\n",
    "The variance of the O1 signal didn't contribute, possibly due to redundancy with other features or lack of discriminative power.\n",
    "4. Overall Analysis and Recommendations\n",
    "Understanding the Model's Behavior:\n",
    "\n",
    "High Recall for Class 0: The model correctly identifies all control group instances but at the cost of misclassifying many amblyopia instances as controls.\n",
    "Low Recall for Class 1: The model misses 60% of the amblyopia cases, which is critical if the goal is to detect amblyopia.\n",
    "Feature Dependence: The model's reliance on extreme values (maximum and minimum) may make it sensitive to noise or outliers in the data.\n",
    "Possible Issues:\n",
    "\n",
    "Data Quality: EEG data can be noisy. Extreme values may be influenced by artifacts rather than true neural activity.\n",
    "Overfitting to Noise: Focusing on maximum and minimum values might cause the model to capture noise rather than meaningful patterns.\n",
    "Small Sample Size: With a limited number of samples, especially in the test set, performance metrics may not be reliable.\n",
    "Feature Selection Limitations: Selecting only 10 features may not capture the complexity needed to differentiate between classes.\n",
    "Recommendations:\n",
    "\n",
    "Increase Dataset Size:\n",
    "\n",
    "Collect more EEG recordings to provide the model with more examples to learn from.\n",
    "Enhance Feature Extraction:\n",
    "\n",
    "Use more comprehensive feature extraction methods, possibly including frequency-domain features (e.g., power spectral density).\n",
    "Consider time-frequency analysis (e.g., wavelet transforms) to capture transient events.\n",
    "Feature Selection Strategy:\n",
    "\n",
    "Instead of selecting a fixed number of features (k), consider using all features or use techniques like recursive feature elimination (RFE) to find the optimal feature subset.\n",
    "Evaluate feature importance using different criteria, such as mutual information.\n",
    "Address Class Imbalance:\n",
    "\n",
    "Although the class distribution isn't severely imbalanced, using techniques like SMOTE (Synthetic Minority Over-sampling Technique) can help the model learn better representations of the minority class.\n",
    "Ensure that the train-test split maintains class distribution (stratified sampling).\n",
    "Model Tuning and Validation:\n",
    "\n",
    "Experiment with different models, such as Gradient Boosting Machines, Support Vector Machines, or Neural Networks.\n",
    "Use cross-validation to obtain a more reliable estimate of the model's performance.\n",
    "Adjust Evaluation Metrics:\n",
    "\n",
    "Since missing amblyopia cases is more critical, consider optimizing for recall on class 1.\n",
    "Use metrics like ROC AUC or Precision-Recall curves to get a better understanding of model performance.\n",
    "Data Preprocessing:\n",
    "\n",
    "Apply signal processing techniques to reduce noise, such as filtering or artifact rejection.\n",
    "Normalize or standardize the features to reduce the impact of scale differences.\n",
    "Investigate Feature Importances:\n",
    "\n",
    "Analyze why certain features are more important.\n",
    "Consider if these features make sense from a neuroscientific perspective.\n",
    "Conclusion\n",
    "Model Limitations: The current model doesn't perform adequately, especially in detecting the amblyopia class, which is critical for your application.\n",
    "Next Steps: Implement the recommendations to improve data quality, feature richness, and model robustness.\n",
    "Continuous Evaluation: As you make changes, continue to evaluate the model using appropriate metrics and validation strategies.\n",
    "Remember: Machine learning in healthcare and biomedical applications often requires careful consideration of data quality, feature engineering, and ethical implications of false negatives and false positives. It's crucial to ensure that the model is reliable and performs well on the aspects that matter most for the intended use case.\n",
    "\n",
    "Let me know if you have any questions or need further clarification on any of these points!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 15/15 [00:01<00:00,  7.60it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:02<00:00,  7.13it/s]\n",
      "Feature Extraction: 100%|██████████| 15/15 [00:01<00:00,  7.97it/s]\n",
      "Feature Extraction: 100%|██████████| 14/14 [00:01<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.67      0.62         6\n",
      "           1       0.60      0.50      0.55         6\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.59      0.58      0.58        12\n",
      "weighted avg       0.59      0.58      0.58        12\n",
      "\n",
      "                  Feature  Importance\n",
      "9             O2__minimum    0.215724\n",
      "6    O2__root_mean_square    0.173315\n",
      "1              Oz__length    0.118186\n",
      "8    O2__absolute_maximum    0.113908\n",
      "4  O2__standard_deviation    0.107788\n",
      "0             O1__minimum    0.088698\n",
      "3              O2__length    0.076589\n",
      "7             O2__maximum    0.055174\n",
      "2             Oz__minimum    0.050617\n",
      "5            O2__variance    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# Define the function to process data in chunks using ComprehensiveFCParameters\n",
    "def process_in_chunks(time_series_df, N):\n",
    "    # Get unique IDs\n",
    "    unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the IDs in the current chunk\n",
    "        chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk using ComprehensiveFCParameters\n",
    "        extracted_features_chunk = extract_features(\n",
    "            chunk_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=MinimalFCParameters(),  # Use ComprehensiveFCParameters() for more features\n",
    "            n_jobs=4,  # Adjust based on your CPU cores\n",
    "            # Since data is in wide format, we do not need to specify column_kind and column_value\n",
    "        )\n",
    "        \n",
    "        # Impute missing values in the extracted features\n",
    "        impute(extracted_features_chunk)\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features_chunk)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_df, extracted_features_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Set the chunk size N (adjust based on your memory constraints)\n",
    "N = 10  # Smaller chunk size to manage memory usage\n",
    "\n",
    "# Extract features using the process_in_chunks function\n",
    "extracted_features = process_in_chunks(time_series_df, N)\n",
    "\n",
    "# Drop any columns with NaN or infinite values\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Ensure that the labels are aligned with the extracted features\n",
    "# Assuming 'labels' is a Series with 'id' as the index\n",
    "labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_features_clean,\n",
    "    labels_aligned,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=labels_aligned  # Ensure stratified sampling\n",
    ")\n",
    "\n",
    "# Select the most important features using ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=10)  # Adjust 'k' as needed\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_depth': [None, 10, 20, 30, 50],\n",
    "    'min_samples_split': [2, 5, 10, 25],\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best estimator from GridSearchCV to predict and evaluate the model\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Identify and display the top selected features with importance\n",
    "important_features = pd.DataFrame({\n",
    "    'Feature': selected_feature_names,\n",
    "    'Importance': best_clf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 15/15 [02:52<00:00, 11.48s/it]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tsfresh\\utilities\\dataframe_functions.py:198: RuntimeWarning: The columns ['O1__autocorrelation__lag_0' 'O1__autocorrelation__lag_1'\n",
      " 'O1__autocorrelation__lag_2' 'O1__autocorrelation__lag_3'\n",
      " 'O1__autocorrelation__lag_4' 'O1__autocorrelation__lag_5'\n",
      " 'O1__autocorrelation__lag_6' 'O1__autocorrelation__lag_7'\n",
      " 'O1__autocorrelation__lag_8' 'O1__autocorrelation__lag_9'\n",
      " 'O1__query_similarity_count__query_None__threshold_0.0'\n",
      " 'Oz__autocorrelation__lag_0' 'Oz__autocorrelation__lag_1'\n",
      " 'Oz__autocorrelation__lag_2' 'Oz__autocorrelation__lag_3'\n",
      " 'Oz__autocorrelation__lag_4' 'Oz__autocorrelation__lag_5'\n",
      " 'Oz__autocorrelation__lag_6' 'Oz__autocorrelation__lag_7'\n",
      " 'Oz__autocorrelation__lag_8' 'Oz__autocorrelation__lag_9'\n",
      " 'Oz__query_similarity_count__query_None__threshold_0.0'\n",
      " 'O2__autocorrelation__lag_0' 'O2__autocorrelation__lag_1'\n",
      " 'O2__autocorrelation__lag_2' 'O2__autocorrelation__lag_3'\n",
      " 'O2__autocorrelation__lag_4' 'O2__autocorrelation__lag_5'\n",
      " 'O2__autocorrelation__lag_6' 'O2__autocorrelation__lag_7'\n",
      " 'O2__autocorrelation__lag_8' 'O2__autocorrelation__lag_9'\n",
      " 'O2__query_similarity_count__query_None__threshold_0.0'] did not have any finite values. Filling with zeros.\n",
      "  warnings.warn(\n",
      "Feature Extraction: 100%|██████████| 15/15 [03:05<00:00, 12.35s/it]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tsfresh\\utilities\\dataframe_functions.py:198: RuntimeWarning: The columns ['O1__autocorrelation__lag_0' 'O1__autocorrelation__lag_1'\n",
      " 'O1__autocorrelation__lag_2' 'O1__autocorrelation__lag_3'\n",
      " 'O1__autocorrelation__lag_4' 'O1__autocorrelation__lag_5'\n",
      " 'O1__autocorrelation__lag_6' 'O1__autocorrelation__lag_7'\n",
      " 'O1__autocorrelation__lag_8' 'O1__autocorrelation__lag_9'\n",
      " 'O1__query_similarity_count__query_None__threshold_0.0'\n",
      " 'Oz__autocorrelation__lag_0' 'Oz__autocorrelation__lag_1'\n",
      " 'Oz__autocorrelation__lag_2' 'Oz__autocorrelation__lag_3'\n",
      " 'Oz__autocorrelation__lag_4' 'Oz__autocorrelation__lag_5'\n",
      " 'Oz__autocorrelation__lag_6' 'Oz__autocorrelation__lag_7'\n",
      " 'Oz__autocorrelation__lag_8' 'Oz__autocorrelation__lag_9'\n",
      " 'Oz__query_similarity_count__query_None__threshold_0.0'\n",
      " 'O2__autocorrelation__lag_0' 'O2__autocorrelation__lag_1'\n",
      " 'O2__autocorrelation__lag_2' 'O2__autocorrelation__lag_3'\n",
      " 'O2__autocorrelation__lag_4' 'O2__autocorrelation__lag_5'\n",
      " 'O2__autocorrelation__lag_6' 'O2__autocorrelation__lag_7'\n",
      " 'O2__autocorrelation__lag_8' 'O2__autocorrelation__lag_9'\n",
      " 'O2__query_similarity_count__query_None__threshold_0.0'] did not have any finite values. Filling with zeros.\n",
      "  warnings.warn(\n",
      "Feature Extraction: 100%|██████████| 15/15 [02:54<00:00, 11.64s/it]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tsfresh\\utilities\\dataframe_functions.py:198: RuntimeWarning: The columns ['Oz__autocorrelation__lag_0' 'Oz__autocorrelation__lag_1'\n",
      " 'Oz__autocorrelation__lag_2' 'Oz__autocorrelation__lag_3'\n",
      " 'Oz__autocorrelation__lag_4' 'Oz__autocorrelation__lag_5'\n",
      " 'Oz__autocorrelation__lag_6' 'Oz__autocorrelation__lag_7'\n",
      " 'Oz__autocorrelation__lag_8' 'Oz__autocorrelation__lag_9'\n",
      " 'Oz__query_similarity_count__query_None__threshold_0.0'\n",
      " 'O2__autocorrelation__lag_0' 'O2__autocorrelation__lag_1'\n",
      " 'O2__autocorrelation__lag_2' 'O2__autocorrelation__lag_3'\n",
      " 'O2__autocorrelation__lag_4' 'O2__autocorrelation__lag_5'\n",
      " 'O2__autocorrelation__lag_6' 'O2__autocorrelation__lag_7'\n",
      " 'O2__autocorrelation__lag_8' 'O2__autocorrelation__lag_9'\n",
      " 'O2__query_similarity_count__query_None__threshold_0.0'\n",
      " 'O1__autocorrelation__lag_0' 'O1__autocorrelation__lag_1'\n",
      " 'O1__autocorrelation__lag_2' 'O1__autocorrelation__lag_3'\n",
      " 'O1__autocorrelation__lag_4' 'O1__autocorrelation__lag_5'\n",
      " 'O1__autocorrelation__lag_6' 'O1__autocorrelation__lag_7'\n",
      " 'O1__autocorrelation__lag_8' 'O1__autocorrelation__lag_9'\n",
      " 'O1__query_similarity_count__query_None__threshold_0.0'] did not have any finite values. Filling with zeros.\n",
      "  warnings.warn(\n",
      "Feature Extraction: 100%|██████████| 14/14 [02:46<00:00, 11.92s/it]\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tsfresh\\utilities\\dataframe_functions.py:198: RuntimeWarning: The columns ['O1__autocorrelation__lag_0' 'O1__autocorrelation__lag_1'\n",
      " 'O1__autocorrelation__lag_2' 'O1__autocorrelation__lag_3'\n",
      " 'O1__autocorrelation__lag_4' 'O1__autocorrelation__lag_5'\n",
      " 'O1__autocorrelation__lag_6' 'O1__autocorrelation__lag_7'\n",
      " 'O1__autocorrelation__lag_8' 'O1__autocorrelation__lag_9'\n",
      " 'O1__query_similarity_count__query_None__threshold_0.0'\n",
      " 'Oz__autocorrelation__lag_0' 'Oz__autocorrelation__lag_1'\n",
      " 'Oz__autocorrelation__lag_2' 'Oz__autocorrelation__lag_3'\n",
      " 'Oz__autocorrelation__lag_4' 'Oz__autocorrelation__lag_5'\n",
      " 'Oz__autocorrelation__lag_6' 'Oz__autocorrelation__lag_7'\n",
      " 'Oz__autocorrelation__lag_8' 'Oz__autocorrelation__lag_9'\n",
      " 'Oz__query_similarity_count__query_None__threshold_0.0'\n",
      " 'O2__autocorrelation__lag_0' 'O2__autocorrelation__lag_1'\n",
      " 'O2__autocorrelation__lag_2' 'O2__autocorrelation__lag_3'\n",
      " 'O2__autocorrelation__lag_4' 'O2__autocorrelation__lag_5'\n",
      " 'O2__autocorrelation__lag_6' 'O2__autocorrelation__lag_7'\n",
      " 'O2__autocorrelation__lag_8' 'O2__autocorrelation__lag_9'\n",
      " 'O2__query_similarity_count__query_None__threshold_0.0'] did not have any finite values. Filling with zeros.\n",
      "  warnings.warn(\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [   0    1    2    3   27   28   29   30   31   45   46   47   48   49\n",
      "   50   51   52   53   54   55   56   57   58   59   60   61   62   63\n",
      "   64   65   68   69   70   71   72   73   74   75   76   77   78   79\n",
      "   80   81   82   83   92   93   94   95   96   97   98   99  100  101\n",
      "  105  365  669  670  671  742  743  762  763  781  783  784  785  786\n",
      "  810  811  812  813  814  828  829  830  831  832  833  834  835  836\n",
      "  837  838  839  840  841  842  843  844  845  846  847  848  851  852\n",
      "  853  854  855  856  857  858  859  860  861  862  863  864  865  866\n",
      "  875  876  877  878  879  880  881  882  883  884  888 1148 1452 1453\n",
      " 1454 1525 1526 1546 1564 1566 1567 1568 1569 1593 1594 1595 1596 1597\n",
      " 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624\n",
      " 1625 1626 1627 1628 1629 1630 1631 1634 1635 1636 1637 1638 1639 1640\n",
      " 1641 1642 1643 1644 1645 1646 1647 1648 1649 1658 1659 1660 1661 1662\n",
      " 1663 1664 1665 1666 1667 1671 1931 2235 2236 2237 2308 2309 2329 2347] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating Random Forest...\n",
      "Best parameters for Random Forest: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.71      0.83      0.77         6\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.76      0.75      0.75        12\n",
      "weighted avg       0.76      0.75      0.75        12\n",
      "\n",
      "Important features for Random Forest:\n",
      "                                             Feature  Importance\n",
      "4          O2__fft_coefficient__attr_\"abs\"__coeff_73    0.252657\n",
      "5     O2__friedrich_coefficients__coeff_1__m_3__r_30    0.197108\n",
      "7  O2__agg_linear_trend__attr_\"intercept\"__chunk_...    0.187262\n",
      "3          O2__fft_coefficient__attr_\"abs\"__coeff_37    0.180176\n",
      "0         O1__fft_coefficient__attr_\"real\"__coeff_94    0.178176\n",
      "2  O2__change_quantiles__f_agg_\"mean\"__isabs_Fals...    0.004622\n",
      "1  O2__change_quantiles__f_agg_\"mean\"__isabs_Fals...    0.000000\n",
      "6     O2__friedrich_coefficients__coeff_3__m_3__r_30    0.000000\n",
      "8  O2__agg_linear_trend__attr_\"stderr\"__chunk_len...    0.000000\n",
      "9  O2__agg_linear_trend__attr_\"stderr\"__chunk_len...    0.000000\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Best parameters for Logistic Regression: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       0.80      0.67      0.73         6\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.76      0.75      0.75        12\n",
      "weighted avg       0.76      0.75      0.75        12\n",
      "\n",
      "Important features for Logistic Regression:\n",
      "                                             Feature    Importance\n",
      "0         O1__fft_coefficient__attr_\"real\"__coeff_94  2.477431e-03\n",
      "5     O2__friedrich_coefficients__coeff_1__m_3__r_30  2.406309e-03\n",
      "4          O2__fft_coefficient__attr_\"abs\"__coeff_73  1.636921e-03\n",
      "3          O2__fft_coefficient__attr_\"abs\"__coeff_37  2.157138e-04\n",
      "7  O2__agg_linear_trend__attr_\"intercept\"__chunk_...  1.570287e-06\n",
      "1  O2__change_quantiles__f_agg_\"mean\"__isabs_Fals...  4.688273e-08\n",
      "2  O2__change_quantiles__f_agg_\"mean\"__isabs_Fals...  4.672234e-08\n",
      "6     O2__friedrich_coefficients__coeff_3__m_3__r_30  8.226773e-09\n",
      "9  O2__agg_linear_trend__attr_\"stderr\"__chunk_len...  5.890940e-10\n",
      "8  O2__agg_linear_trend__attr_\"stderr\"__chunk_len...  4.074925e-11\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.67      0.62         6\n",
      "           1       0.60      0.50      0.55         6\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.59      0.58      0.58        12\n",
      "weighted avg       0.59      0.58      0.58        12\n",
      "\n",
      "Important features for Gradient Boosting:\n",
      "                                             Feature  Importance\n",
      "7  O2__agg_linear_trend__attr_\"intercept\"__chunk_...        0.49\n",
      "0         O1__fft_coefficient__attr_\"real\"__coeff_94        0.26\n",
      "5     O2__friedrich_coefficients__coeff_1__m_3__r_30        0.25\n",
      "1  O2__change_quantiles__f_agg_\"mean\"__isabs_Fals...        0.00\n",
      "2  O2__change_quantiles__f_agg_\"mean\"__isabs_Fals...        0.00\n",
      "3          O2__fft_coefficient__attr_\"abs\"__coeff_37        0.00\n",
      "4          O2__fft_coefficient__attr_\"abs\"__coeff_73        0.00\n",
      "6     O2__friedrich_coefficients__coeff_3__m_3__r_30        0.00\n",
      "8  O2__agg_linear_trend__attr_\"stderr\"__chunk_len...        0.00\n",
      "9  O2__agg_linear_trend__attr_\"stderr\"__chunk_len...        0.00\n",
      "\n",
      "Training and evaluating Neural Network...\n",
      "Best parameters for Neural Network: {'classifier__activation': 'tanh', 'classifier__alpha': 0.0001, 'classifier__hidden_layer_sizes': (50,), 'classifier__learning_rate': 'constant', 'classifier__solver': 'adam'}\n",
      "Classification Report for Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         6\n",
      "           1       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.80      0.67      0.62        12\n",
      "weighted avg       0.80      0.67      0.62        12\n",
      "\n",
      "Neural Network does not provide feature importances directly.\n",
      "\n",
      "Training and evaluating XGBoost...\n",
      "Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "Classification Report for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.83      0.59         6\n",
      "           1       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.23      0.42      0.29        12\n",
      "weighted avg       0.23      0.42      0.29        12\n",
      "\n",
      "Important features for XGBoost:\n",
      "                                             Feature  Importance\n",
      "5     O2__friedrich_coefficients__coeff_1__m_3__r_30    0.247380\n",
      "0         O1__fft_coefficient__attr_\"real\"__coeff_94    0.223214\n",
      "1  O2__change_quantiles__f_agg_\"mean\"__isabs_Fals...    0.213562\n",
      "2  O2__change_quantiles__f_agg_\"mean\"__isabs_Fals...    0.168974\n",
      "7  O2__agg_linear_trend__attr_\"intercept\"__chunk_...    0.052321\n",
      "3          O2__fft_coefficient__attr_\"abs\"__coeff_37    0.047926\n",
      "6     O2__friedrich_coefficients__coeff_3__m_3__r_30    0.046623\n",
      "4          O2__fft_coefficient__attr_\"abs\"__coeff_73    0.000000\n",
      "8  O2__agg_linear_trend__attr_\"stderr\"__chunk_len...    0.000000\n",
      "9  O2__agg_linear_trend__attr_\"stderr\"__chunk_len...    0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WERPELGA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:09:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier  # Import XGBoost classifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# Define the function to process data in chunks using ComprehensiveFCParameters\n",
    "def process_in_chunks(time_series_df, N):\n",
    "    # Get unique IDs\n",
    "    unique_ids = time_series_df['id'].unique()\n",
    "    \n",
    "    # Split the unique IDs into chunks of size N\n",
    "    chunks = [unique_ids[i:i + N] for i in range(0, len(unique_ids), N)]\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        # Filter the DataFrame to include only the IDs in the current chunk\n",
    "        chunk_df = time_series_df[time_series_df['id'].isin(chunk)]\n",
    "        \n",
    "        # Extract features for the current chunk using ComprehensiveFCParameters\n",
    "        extracted_features_chunk = extract_features(\n",
    "            chunk_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=ComprehensiveFCParameters(),  # Use ComprehensiveFCParameters() for more features\n",
    "            n_jobs=4,  # Adjust based on your CPU cores\n",
    "            # Since data is in wide format, we do not need to specify column_kind and column_value\n",
    "        )\n",
    "        \n",
    "        # Impute missing values in the extracted features\n",
    "        impute(extracted_features_chunk)\n",
    "        \n",
    "        # Append the extracted features to the results list\n",
    "        results.append(extracted_features_chunk)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_df, extracted_features_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    final_result = pd.concat(results)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Set the chunk size N (adjust based on your memory constraints)\n",
    "N = 10  # Smaller chunk size to manage memory usage\n",
    "\n",
    "# Extract features using the process_in_chunks function\n",
    "extracted_features = process_in_chunks(time_series_df, N)\n",
    "\n",
    "# Drop any columns with NaN or infinite values\n",
    "extracted_features_clean = extracted_features.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Ensure that the labels are aligned with the extracted features\n",
    "# Assuming 'labels' is a Series with 'id' as the index\n",
    "labels_aligned = labels.loc[extracted_features_clean.index]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_features_clean,\n",
    "    labels_aligned,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=labels_aligned  # Ensure stratified sampling\n",
    ")\n",
    "\n",
    "# Select the most important features using ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=10)  # Adjust 'k' as needed\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = extracted_features_clean.columns[selector.get_support()]\n",
    "\n",
    "# Initialize an empty dictionary to store classifiers and their parameter grids\n",
    "classifiers = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=5000),\n",
    "        'param_grid': {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['liblinear'],\n",
    "        }\n",
    "    },\n",
    "    # 'Support Vector Machine': {\n",
    "    #     'model': SVC(random_state=42),\n",
    "    #     'param_grid': [\n",
    "    #         {'kernel': ['linear'], 'C': [0.1, 1, 10, 100]},\n",
    "    #         {'kernel': ['rbf'], 'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto']},\n",
    "    #         {'kernel': ['poly'], 'C': [0.1, 1, 10], 'degree': [2, 3], 'gamma': ['scale', 'auto']}\n",
    "    #     ]\n",
    "    # },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5],\n",
    "            'min_samples_split': [2, 5],\n",
    "        }\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'model': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', MLPClassifier(random_state=42, max_iter=500))\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'classifier__activation': ['tanh', 'relu'],\n",
    "            'classifier__solver': ['adam', 'sgd'],\n",
    "            'classifier__alpha': [0.0001, 0.001],\n",
    "            'classifier__learning_rate': ['constant', 'adaptive'],\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200, 300, 500],\n",
    "            'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.5, 0.9],\n",
    "            'subsample': [0.5, 0.7, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.5, 0.7, 0.8, 1.0],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop through each classifier, perform grid search, and evaluate\n",
    "for name, classifier_info in classifiers.items():\n",
    "    print(f\"\\nTraining and evaluating {name}...\")\n",
    "    model = classifier_info['model']\n",
    "    param_grid = classifier_info['param_grid']\n",
    "    \n",
    "    # For classifiers that include the feature selection or scaling in a pipeline, use X_train and X_test directly\n",
    "    if name == 'Neural Network':\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_clf = grid_search.best_estimator_\n",
    "        y_pred = best_clf.predict(X_test)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        best_clf = grid_search.best_estimator_\n",
    "        y_pred = best_clf.predict(X_test_selected)\n",
    "    \n",
    "    # Print the best parameters found by GridSearchCV\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # For models that provide feature importances, display them\n",
    "    if hasattr(best_clf, 'feature_importances_'):\n",
    "        important_features = pd.DataFrame({\n",
    "            'Feature': selected_feature_names,\n",
    "            'Importance': best_clf.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(f\"Important features for {name}:\")\n",
    "        print(important_features)\n",
    "    elif hasattr(best_clf, 'coef_'):\n",
    "        # For linear models like Logistic Regression\n",
    "        importance = np.abs(best_clf.coef_[0])\n",
    "        important_features = pd.DataFrame({\n",
    "            'Feature': selected_feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(f\"Important features for {name}:\")\n",
    "        print(important_features)\n",
    "    else:\n",
    "        print(f\"{name} does not provide feature importances directly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'best_clf' is your best XGBoost classifier from GridSearchCV\n",
    "# if name == 'XGBoost':\n",
    "#     # Save the best XGBoost model using joblib\n",
    "#     joblib.dump(best_clf, 'best_xgboost_model.pkl')\n",
    "#     print(\"XGBoost model saved to 'best_xgboost_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# # Save the trained classifier\n",
    "# joblib.dump(best_clf, 'trained_random_forest.pkl')\n",
    "\n",
    "# # Save the feature selector\n",
    "# joblib.dump(selector, 'feature_selector.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
